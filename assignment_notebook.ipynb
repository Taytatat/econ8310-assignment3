{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e649671-64ae-4692-a381-33974ffa666a",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## Econ 8310 - Business Forecasting\n",
    "\n",
    "For homework assignment 3, you will work with [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist), a more fancier data set.\n",
    "\n",
    "- You must create a custom data loader as described in the first week of neural network lectures [2 points]\n",
    "    - You will NOT receive credit for this if you use the pytorch prebuilt loader for Fashion MNIST!\n",
    "- You must create a working and trained neural network using only pytorch [2 points]\n",
    "- You must store your weights and create an import script so that I can evaluate your model without training it [2 points]\n",
    "\n",
    "Highest accuracy score gets some extra credit!\n",
    "\n",
    "Submit your forked repository URL on Canvas! :) I'll be manually grading this assignment.\n",
    "\n",
    "Some checks you can make on your own:\n",
    "- Did you manually process the data or use a prebuilt loader (see above)?\n",
    "- Does your script train a neural network on the assigned data?\n",
    "- Did your script save your model?\n",
    "- Do you have separate code to import your model for use after training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcd35387-b05b-4cb2-9b1f-e1d2c0e43588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install torch torchvision torchaudio <- this is already installed on colab but for other coding envirormnts we would use pip to install these\n",
    "\n",
    "#in positron instal as follows in the console to get pytorch:\n",
    "#%pip install torch torchvision torchaudio\n",
    "\n",
    "# For reading data\n",
    "import gzip\n",
    "import numpy as np\n",
    "import struct\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# For visualizing\n",
    "import plotly.express as px\n",
    "\n",
    "# For model building\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#as we use neurel nets we cant just use panda data frames.\n",
    "#we also need to use mini batches to minimize memory use (read in mini batches of data)\n",
    "\n",
    "#we have to make a data object unique to the data were using becasse data will have different unique shapes depenidng on the data we use\n",
    "#were going to have to create a specififc data class for every set of data (this may or may not make neural networs worth using becuase they will take A LOt of effort)\n",
    "#this is the main reason we cant just use pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d22b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataset(Dataset):\n",
    "    def __init__(self, gz_image_file, gz_label_file, transform=None):\n",
    "        with gzip.open(gz_image_file, 'rb') as f:\n",
    "            magic, num_images = struct.unpack(\">II\", f.read(8))\n",
    "            rows, cols = struct.unpack(\">II\", f.read(8))\n",
    "            self.images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "\n",
    "        with gzip.open(gz_label_file, 'rb') as f:\n",
    "            magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "            self.labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        #this will reshape the image to (1, 28, 28) before returning it back\n",
    "        image = image.reshape(1, 28, 28)\n",
    "        # this will make the image a float32 tensor, yayyyyyy (finally!)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9128b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FashionMNISTDataset(\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\")  # Pass both image and label files\n",
    "test_data = FashionMNISTDataset(\"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\")   # Pass both image and label files\n",
    "\n",
    "# Create data feed pipelines for modeling, this will take our custom data object and sample them in small batches\n",
    "train_dataloader = DataLoader(train_data, batch_size=64) #this will get them ready for our neural network, think of data loaders as the pipeline that feed the data into our machine (the neural network)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eed117f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for some reason this keeps crashing my positiron So STOP EUNNING THIS I GUESS\n",
    "\n",
    "#trying to make code that will create lables for the clothing items (idk if i can use this because it uses the preset from pytorhc?? I cant really tell)\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#labels_map={\n",
    "   # 0: 'T-shirt',\n",
    "   # 1: 'Trouser',\n",
    "   # 2: 'Pullover',\n",
    "   # 3: 'Dress',\n",
    "    #4: 'Coat',\n",
    "    #5: 'Sandal',\n",
    "   # 6: 'Shirt',\n",
    "   # 7: 'Sneaker',\n",
    "   # 8: 'Bag',\n",
    "   # 9: 'Ankle Boot',\n",
    "#}\n",
    "#figure = plt.figure(figsize = (10,10))\n",
    "#cols, rows = 3, 3\n",
    "\n",
    "#for i in range (1, cols*rows + 1):\n",
    "    #sample_idx = torch.randint(len(train_data), size = (1,)).item()\n",
    "    #image, label = train_data[sample_idx]\n",
    "   # figure.add_subplot(rows, cols, i)\n",
    "   # plt.title(labels_map[label])\n",
    "    #plt.axis('off')\n",
    " #   plt.imshow(image.squeeze(), cmap='gray')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbd03f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is image is labeled a T-shirt\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           0,
           41,
           188,
           103,
           54,
           48,
           43,
           87,
           168,
           133,
           16,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           1,
           0,
           0,
           0,
           49,
           136,
           219,
           216,
           228,
           236,
           255,
           255,
           255,
           255,
           217,
           215,
           254,
           231,
           160,
           45,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           14,
           176,
           222,
           224,
           212,
           203,
           198,
           196,
           200,
           215,
           204,
           202,
           201,
           201,
           201,
           209,
           218,
           224,
           164,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           188,
           219,
           200,
           198,
           202,
           198,
           199,
           199,
           201,
           196,
           198,
           198,
           200,
           200,
           200,
           200,
           201,
           200,
           225,
           41,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           51,
           219,
           199,
           203,
           203,
           212,
           238,
           248,
           250,
           245,
           249,
           246,
           247,
           252,
           248,
           235,
           207,
           203,
           203,
           222,
           140,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           116,
           226,
           206,
           204,
           207,
           204,
           101,
           75,
           47,
           73,
           48,
           50,
           45,
           51,
           63,
           113,
           222,
           202,
           206,
           220,
           224,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           200,
           222,
           209,
           203,
           215,
           200,
           0,
           70,
           98,
           0,
           103,
           59,
           68,
           71,
           49,
           0,
           219,
           206,
           214,
           210,
           250,
           38,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           247,
           218,
           212,
           210,
           215,
           214,
           0,
           254,
           243,
           139,
           255,
           174,
           251,
           255,
           205,
           0,
           215,
           217,
           214,
           208,
           220,
           95,
           0,
           0
          ],
          [
           0,
           0,
           0,
           45,
           226,
           214,
           214,
           215,
           224,
           205,
           0,
           42,
           35,
           60,
           16,
           17,
           12,
           13,
           70,
           0,
           189,
           216,
           212,
           206,
           212,
           156,
           0,
           0
          ],
          [
           0,
           0,
           0,
           164,
           235,
           214,
           211,
           220,
           216,
           201,
           52,
           71,
           89,
           94,
           83,
           78,
           70,
           76,
           92,
           87,
           206,
           207,
           222,
           213,
           219,
           208,
           0,
           0
          ],
          [
           0,
           0,
           0,
           106,
           187,
           223,
           237,
           248,
           211,
           198,
           252,
           250,
           248,
           245,
           248,
           252,
           253,
           250,
           252,
           239,
           201,
           212,
           225,
           215,
           193,
           113,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           17,
           54,
           159,
           222,
           193,
           208,
           192,
           197,
           200,
           200,
           200,
           200,
           201,
           203,
           195,
           210,
           165,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           47,
           225,
           192,
           214,
           203,
           206,
           204,
           204,
           205,
           206,
           204,
           212,
           197,
           218,
           107,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           1,
           6,
           0,
           46,
           212,
           195,
           212,
           202,
           206,
           205,
           204,
           205,
           206,
           204,
           212,
           200,
           218,
           91,
           0,
           3,
           1,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           11,
           197,
           199,
           205,
           202,
           205,
           206,
           204,
           205,
           207,
           204,
           205,
           205,
           218,
           77,
           0,
           5,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           3,
           0,
           2,
           191,
           198,
           201,
           205,
           206,
           205,
           205,
           206,
           209,
           206,
           199,
           209,
           219,
           74,
           0,
           5,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           2,
           0,
           0,
           188,
           197,
           200,
           207,
           207,
           204,
           207,
           207,
           210,
           208,
           198,
           207,
           221,
           72,
           0,
           4,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           2,
           0,
           0,
           215,
           198,
           203,
           206,
           208,
           205,
           207,
           207,
           210,
           208,
           200,
           202,
           222,
           75,
           0,
           4,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           212,
           198,
           209,
           206,
           209,
           206,
           208,
           207,
           211,
           206,
           205,
           198,
           221,
           80,
           0,
           3,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           204,
           201,
           205,
           208,
           207,
           205,
           211,
           205,
           210,
           210,
           209,
           195,
           221,
           96,
           0,
           3,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           202,
           201,
           205,
           209,
           207,
           205,
           213,
           206,
           210,
           209,
           210,
           194,
           217,
           105,
           0,
           2,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           204,
           204,
           205,
           208,
           207,
           205,
           215,
           207,
           210,
           208,
           211,
           193,
           213,
           115,
           0,
           2,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           204,
           207,
           207,
           208,
           206,
           206,
           215,
           210,
           210,
           207,
           212,
           195,
           210,
           118,
           0,
           2,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           198,
           208,
           208,
           208,
           204,
           207,
           212,
           212,
           210,
           207,
           211,
           196,
           207,
           121,
           0,
           1,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           198,
           210,
           207,
           208,
           206,
           209,
           213,
           212,
           211,
           207,
           210,
           197,
           207,
           124,
           0,
           1,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           172,
           210,
           203,
           201,
           199,
           204,
           207,
           205,
           204,
           201,
           205,
           197,
           206,
           127,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           188,
           221,
           214,
           234,
           236,
           238,
           244,
           244,
           244,
           240,
           243,
           214,
           224,
           162,
           0,
           2,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           1,
           0,
           0,
           139,
           146,
           130,
           135,
           135,
           137,
           125,
           124,
           125,
           121,
           119,
           114,
           130,
           76,
           0,
           0,
           0,
           0,
           0,
           0
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels_map={\n",
    "    0: 'T-shirt',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle Boot',\n",
    "}\n",
    "\n",
    "idx=1  #as we go through the different index observations we should get back the image and it should tell us what it is (see both image and the value)\n",
    "\n",
    "#want get image labels instead of numbers (like from the in class example)\n",
    "\n",
    "image, label = train_data.__getitem__(idx)\n",
    "\n",
    "#grab the label from the labels_map created above\n",
    "\n",
    "text_label = labels_map[label]\n",
    "\n",
    "#print out the answer::\n",
    "\n",
    "print(f\"This is image is labeled a {text_label}\")\n",
    "px.imshow(image.reshape(28, 28))\n",
    "\n",
    "#got rid of theses because these will give us numbers intead of text (matching)\n",
    "#print(f\"This image is labeled a {train_data.__getitem__(idx)[1]}\")\n",
    "#px.imshow(train_data.__getitem__(idx)[0].reshape(28, 28))\n",
    "\n",
    "\n",
    "#I think I got it right for the most part! Yayyyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5d1faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets build our first network\n",
    "\n",
    "class FirstNet(nn.Module):  #inherheriting from out neural network module (nn) from above\n",
    "    def __init__(self):  #we create our initialization function\n",
    "\n",
    "      # We define the components of our model here\n",
    "      super(FirstNet, self).__init__() #were goinging to initalize with all of the values that come with the nn.Module object (this is where we get all of the initalization stuff already built in by that objecy)\n",
    "\n",
    "      # Function to flatten our image\n",
    "      self.flatten = nn.Flatten()  #then we create a flatten object, using pytorch's flatten object\n",
    "\n",
    "\n",
    "\n",
    "      # Create the sequence of our network\n",
    "      self.linear_relu_model = nn.Sequential(    #now create a linear model, this will be a sequentialled model from our nn library with a single in it (10 perceptrons)\n",
    "            # Add a linear output layer w/ 10 perceptrons\n",
    "            nn.LazyLinear(10),   #the 10 perceptrons look at all the inputs and do their best guessing and combine their info, this will be the layer that gets us our 10 classes. Each neuron represents a different class. Here we wil learn about which pixels matter more for which class of number (0 through 9). Each lable gets its own neuron in this output layer. See how likely each of these outcomes are\n",
    "        )\n",
    "\n",
    "#next we define our forward function (i)\n",
    "\n",
    "    def forward(self, x): #x are our inputs, so this is where the data will flow into this forward network\n",
    "      # We construct the sequencing of our model here\n",
    "      x = self.flatten(x) #x comes in and the we update x to flatten it back out in 748 pixels (note this after we made it 1x28x28 above)\n",
    "      # Pass flattened images through our sequence\n",
    "      output = self.linear_relu_model(x) #then feed the xs through our linear model here, and our model is those 10 perceptrons we talked about above\n",
    "\n",
    "      # Return the evaluations of our ten\n",
    "      #   classes as a 10-dimensional vector\n",
    "      return output #we then pass those 10 different values as output\n",
    "\n",
    "# Create an instance of our model (our class that we created)\n",
    "model = FirstNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a75846bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we start training our neural network (this is going to take a while)\n",
    "\n",
    "# Define some training parameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 17 #every epoch means we pass every observation to our model 1 times, so if we do 20, then our model will have seen the data 20 times\n",
    "\n",
    "# Define our loss function\n",
    "#   This one works for multiclass problems\n",
    "loss_fn = nn.CrossEntropyLoss() #loss function is for how we grade our performance , cross entropy loss is good for multiclass problems. use this all as ameasure of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d16a4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we need to build an optimizer\n",
    "# Build our optimizer with the parameters from\n",
    "#   the model we defined, and the learning rate\n",
    "#   that we picked\n",
    "optimizer = torch.optim.SGD(model.parameters(), #stotastic geadidi desecent model. Take model parameters and learning  rate . optimizer chooses how to advance our model\n",
    "     lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b96f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we prepare to train the model\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) #check how big our datat set is\n",
    "    # Set the model to training mode\n",
    "    # important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train() #set our model into trainig mode, when testing we stop updating our model (stop back poprgation we dont want model to be updating)\n",
    "    # Loop over batches via the dataloader\n",
    "    for batch, (X, y) in enumerate(dataloader): #loop over our batches in the data loader\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X) #make predictions based on the current model\n",
    "        loss = loss_fn(pred, y) #calcualte loss in our model based on those predictions\n",
    "\n",
    "        # Backpropagation and looking for improved gradients\n",
    "        loss.backward() #now go backwards, look for best way to change our model\n",
    "        optimizer.step() #step our model in that direction\n",
    "        # Zeroing out the gradient (otherwise they are summed)\n",
    "        #   in preparation for next round\n",
    "        optimizer.zero_grad() #zero out our optimizier\n",
    "\n",
    "        # Print progress update every few loops (every 10 batches, print out what has happened so far)\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\") #will go through this loop over and over again with each batch, epoch (our training loop is a single epocoh )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b476f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now also need a test loop\n",
    "#note that we dont have an optimizer here (because were not updating the model, instead we set it to evaluation mode)\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode\n",
    "    # important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0 #looks at how many observations we get correct\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures\n",
    "    # that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations\n",
    "    # and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad(): #not looking for gradient\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X) #go through and make predictions\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # Printing some output after a testing round\n",
    "    test_loss /= num_batches #this will give our average output (loss divided by the number of batches )\n",
    "    correct /= size #how many we got right as a % of the number of observations\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c1a0e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 78.164467  [   64/60000]\n",
      "loss: 457.815216  [  704/60000]\n",
      "loss: 348.436157  [ 1344/60000]\n",
      "loss: 164.200531  [ 1984/60000]\n",
      "loss: 208.609772  [ 2624/60000]\n",
      "loss: 310.296173  [ 3264/60000]\n",
      "loss: 278.496521  [ 3904/60000]\n",
      "loss: 315.659210  [ 4544/60000]\n",
      "loss: 232.030060  [ 5184/60000]\n",
      "loss: 118.448746  [ 5824/60000]\n",
      "loss: 107.756783  [ 6464/60000]\n",
      "loss: 194.056946  [ 7104/60000]\n",
      "loss: 302.158905  [ 7744/60000]\n",
      "loss: 287.253662  [ 8384/60000]\n",
      "loss: 174.225723  [ 9024/60000]\n",
      "loss: 138.103348  [ 9664/60000]\n",
      "loss: 186.455124  [10304/60000]\n",
      "loss: 163.132004  [10944/60000]\n",
      "loss: 143.609146  [11584/60000]\n",
      "loss: 344.551758  [12224/60000]\n",
      "loss: 112.869659  [12864/60000]\n",
      "loss: 190.979065  [13504/60000]\n",
      "loss: 319.531677  [14144/60000]\n",
      "loss: 191.886353  [14784/60000]\n",
      "loss: 106.000183  [15424/60000]\n",
      "loss: 237.722946  [16064/60000]\n",
      "loss: 99.242790  [16704/60000]\n",
      "loss: 148.179703  [17344/60000]\n",
      "loss: 148.957794  [17984/60000]\n",
      "loss: 177.826126  [18624/60000]\n",
      "loss: 160.963394  [19264/60000]\n",
      "loss: 171.374527  [19904/60000]\n",
      "loss: 159.645508  [20544/60000]\n",
      "loss: 69.661194  [21184/60000]\n",
      "loss: 415.434326  [21824/60000]\n",
      "loss: 131.493591  [22464/60000]\n",
      "loss: 305.616577  [23104/60000]\n",
      "loss: 115.373306  [23744/60000]\n",
      "loss: 273.804596  [24384/60000]\n",
      "loss: 101.399132  [25024/60000]\n",
      "loss: 336.208008  [25664/60000]\n",
      "loss: 236.983200  [26304/60000]\n",
      "loss: 101.518326  [26944/60000]\n",
      "loss: 221.984955  [27584/60000]\n",
      "loss: 78.985672  [28224/60000]\n",
      "loss: 103.201683  [28864/60000]\n",
      "loss: 139.175217  [29504/60000]\n",
      "loss: 317.583038  [30144/60000]\n",
      "loss: 107.836075  [30784/60000]\n",
      "loss: 72.159058  [31424/60000]\n",
      "loss: 76.016045  [32064/60000]\n",
      "loss: 93.720787  [32704/60000]\n",
      "loss: 123.717842  [33344/60000]\n",
      "loss: 179.068588  [33984/60000]\n",
      "loss: 69.638481  [34624/60000]\n",
      "loss: 105.261002  [35264/60000]\n",
      "loss: 104.184128  [35904/60000]\n",
      "loss: 196.393036  [36544/60000]\n",
      "loss: 225.759674  [37184/60000]\n",
      "loss: 98.441040  [37824/60000]\n",
      "loss: 120.800621  [38464/60000]\n",
      "loss: 141.670319  [39104/60000]\n",
      "loss: 274.007599  [39744/60000]\n",
      "loss: 305.452118  [40384/60000]\n",
      "loss: 247.328400  [41024/60000]\n",
      "loss: 288.556610  [41664/60000]\n",
      "loss: 119.040062  [42304/60000]\n",
      "loss: 171.102310  [42944/60000]\n",
      "loss: 124.061401  [43584/60000]\n",
      "loss: 201.046082  [44224/60000]\n",
      "loss: 139.662445  [44864/60000]\n",
      "loss: 45.419312  [45504/60000]\n",
      "loss: 145.465424  [46144/60000]\n",
      "loss: 106.245590  [46784/60000]\n",
      "loss: 208.750305  [47424/60000]\n",
      "loss: 114.600906  [48064/60000]\n",
      "loss: 106.819458  [48704/60000]\n",
      "loss: 180.455261  [49344/60000]\n",
      "loss: 175.919312  [49984/60000]\n",
      "loss: 131.729431  [50624/60000]\n",
      "loss: 211.259979  [51264/60000]\n",
      "loss: 397.233704  [51904/60000]\n",
      "loss: 252.486465  [52544/60000]\n",
      "loss: 90.791481  [53184/60000]\n",
      "loss: 115.469421  [53824/60000]\n",
      "loss: 89.453835  [54464/60000]\n",
      "loss: 118.526237  [55104/60000]\n",
      "loss: 65.785645  [55744/60000]\n",
      "loss: 143.981293  [56384/60000]\n",
      "loss: 122.116898  [57024/60000]\n",
      "loss: 128.537430  [57664/60000]\n",
      "loss: 47.592743  [58304/60000]\n",
      "loss: 85.123039  [58944/60000]\n",
      "loss: 271.470001  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 206.458186 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 149.701248  [   64/60000]\n",
      "loss: 158.139648  [  704/60000]\n",
      "loss: 67.529129  [ 1344/60000]\n",
      "loss: 128.606964  [ 1984/60000]\n",
      "loss: 121.048454  [ 2624/60000]\n",
      "loss: 106.378387  [ 3264/60000]\n",
      "loss: 133.809494  [ 3904/60000]\n",
      "loss: 99.645020  [ 4544/60000]\n",
      "loss: 154.970993  [ 5184/60000]\n",
      "loss: 106.860420  [ 5824/60000]\n",
      "loss: 59.316280  [ 6464/60000]\n",
      "loss: 56.638206  [ 7104/60000]\n",
      "loss: 211.563705  [ 7744/60000]\n",
      "loss: 201.680313  [ 8384/60000]\n",
      "loss: 168.790375  [ 9024/60000]\n",
      "loss: 79.757980  [ 9664/60000]\n",
      "loss: 84.812897  [10304/60000]\n",
      "loss: 102.944748  [10944/60000]\n",
      "loss: 104.795692  [11584/60000]\n",
      "loss: 178.463654  [12224/60000]\n",
      "loss: 141.372650  [12864/60000]\n",
      "loss: 245.294739  [13504/60000]\n",
      "loss: 34.190495  [14144/60000]\n",
      "loss: 96.362549  [14784/60000]\n",
      "loss: 125.643845  [15424/60000]\n",
      "loss: 101.606804  [16064/60000]\n",
      "loss: 57.407742  [16704/60000]\n",
      "loss: 136.431625  [17344/60000]\n",
      "loss: 149.626175  [17984/60000]\n",
      "loss: 68.696548  [18624/60000]\n",
      "loss: 108.989159  [19264/60000]\n",
      "loss: 158.361649  [19904/60000]\n",
      "loss: 56.931351  [20544/60000]\n",
      "loss: 97.709480  [21184/60000]\n",
      "loss: 228.048523  [21824/60000]\n",
      "loss: 164.651459  [22464/60000]\n",
      "loss: 223.447372  [23104/60000]\n",
      "loss: 117.449211  [23744/60000]\n",
      "loss: 143.813126  [24384/60000]\n",
      "loss: 76.830017  [25024/60000]\n",
      "loss: 124.078629  [25664/60000]\n",
      "loss: 265.674774  [26304/60000]\n",
      "loss: 120.217903  [26944/60000]\n",
      "loss: 210.886841  [27584/60000]\n",
      "loss: 69.989304  [28224/60000]\n",
      "loss: 82.096642  [28864/60000]\n",
      "loss: 179.467819  [29504/60000]\n",
      "loss: 284.585358  [30144/60000]\n",
      "loss: 139.697510  [30784/60000]\n",
      "loss: 91.083397  [31424/60000]\n",
      "loss: 165.662750  [32064/60000]\n",
      "loss: 112.125389  [32704/60000]\n",
      "loss: 90.747406  [33344/60000]\n",
      "loss: 208.879700  [33984/60000]\n",
      "loss: 55.023586  [34624/60000]\n",
      "loss: 57.324471  [35264/60000]\n",
      "loss: 104.594452  [35904/60000]\n",
      "loss: 144.328186  [36544/60000]\n",
      "loss: 152.796661  [37184/60000]\n",
      "loss: 69.731804  [37824/60000]\n",
      "loss: 109.850601  [38464/60000]\n",
      "loss: 184.339722  [39104/60000]\n",
      "loss: 105.298920  [39744/60000]\n",
      "loss: 192.801376  [40384/60000]\n",
      "loss: 147.226135  [41024/60000]\n",
      "loss: 95.363670  [41664/60000]\n",
      "loss: 180.136261  [42304/60000]\n",
      "loss: 111.110031  [42944/60000]\n",
      "loss: 286.993591  [43584/60000]\n",
      "loss: 196.639801  [44224/60000]\n",
      "loss: 131.316422  [44864/60000]\n",
      "loss: 117.202866  [45504/60000]\n",
      "loss: 93.637947  [46144/60000]\n",
      "loss: 82.354904  [46784/60000]\n",
      "loss: 142.570221  [47424/60000]\n",
      "loss: 85.168106  [48064/60000]\n",
      "loss: 101.739563  [48704/60000]\n",
      "loss: 85.540833  [49344/60000]\n",
      "loss: 132.167664  [49984/60000]\n",
      "loss: 136.415405  [50624/60000]\n",
      "loss: 263.637146  [51264/60000]\n",
      "loss: 446.999359  [51904/60000]\n",
      "loss: 491.585602  [52544/60000]\n",
      "loss: 99.594765  [53184/60000]\n",
      "loss: 143.886154  [53824/60000]\n",
      "loss: 139.417892  [54464/60000]\n",
      "loss: 113.584229  [55104/60000]\n",
      "loss: 61.953201  [55744/60000]\n",
      "loss: 71.492096  [56384/60000]\n",
      "loss: 84.081238  [57024/60000]\n",
      "loss: 81.216522  [57664/60000]\n",
      "loss: 113.863487  [58304/60000]\n",
      "loss: 68.652061  [58944/60000]\n",
      "loss: 201.559494  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 255.722937 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 187.873169  [   64/60000]\n",
      "loss: 108.091019  [  704/60000]\n",
      "loss: 85.933609  [ 1344/60000]\n",
      "loss: 111.783112  [ 1984/60000]\n",
      "loss: 168.013977  [ 2624/60000]\n",
      "loss: 77.681038  [ 3264/60000]\n",
      "loss: 140.887329  [ 3904/60000]\n",
      "loss: 95.764359  [ 4544/60000]\n",
      "loss: 216.303116  [ 5184/60000]\n",
      "loss: 175.286301  [ 5824/60000]\n",
      "loss: 51.273117  [ 6464/60000]\n",
      "loss: 114.284454  [ 7104/60000]\n",
      "loss: 83.475914  [ 7744/60000]\n",
      "loss: 248.155518  [ 8384/60000]\n",
      "loss: 196.400269  [ 9024/60000]\n",
      "loss: 85.964355  [ 9664/60000]\n",
      "loss: 202.142868  [10304/60000]\n",
      "loss: 166.361969  [10944/60000]\n",
      "loss: 110.134293  [11584/60000]\n",
      "loss: 159.475433  [12224/60000]\n",
      "loss: 421.187622  [12864/60000]\n",
      "loss: 83.060287  [13504/60000]\n",
      "loss: 54.768349  [14144/60000]\n",
      "loss: 123.627693  [14784/60000]\n",
      "loss: 68.960175  [15424/60000]\n",
      "loss: 148.333542  [16064/60000]\n",
      "loss: 138.358368  [16704/60000]\n",
      "loss: 113.354622  [17344/60000]\n",
      "loss: 166.616226  [17984/60000]\n",
      "loss: 65.191086  [18624/60000]\n",
      "loss: 155.180099  [19264/60000]\n",
      "loss: 212.769226  [19904/60000]\n",
      "loss: 47.547523  [20544/60000]\n",
      "loss: 77.454826  [21184/60000]\n",
      "loss: 293.814270  [21824/60000]\n",
      "loss: 159.384644  [22464/60000]\n",
      "loss: 225.082123  [23104/60000]\n",
      "loss: 81.789375  [23744/60000]\n",
      "loss: 226.733887  [24384/60000]\n",
      "loss: 80.162521  [25024/60000]\n",
      "loss: 93.918541  [25664/60000]\n",
      "loss: 249.840027  [26304/60000]\n",
      "loss: 99.130379  [26944/60000]\n",
      "loss: 83.713257  [27584/60000]\n",
      "loss: 63.105030  [28224/60000]\n",
      "loss: 43.566383  [28864/60000]\n",
      "loss: 279.058746  [29504/60000]\n",
      "loss: 209.421753  [30144/60000]\n",
      "loss: 80.175903  [30784/60000]\n",
      "loss: 82.771942  [31424/60000]\n",
      "loss: 94.818146  [32064/60000]\n",
      "loss: 109.966057  [32704/60000]\n",
      "loss: 28.919437  [33344/60000]\n",
      "loss: 138.124985  [33984/60000]\n",
      "loss: 63.423233  [34624/60000]\n",
      "loss: 53.390141  [35264/60000]\n",
      "loss: 70.083351  [35904/60000]\n",
      "loss: 99.925873  [36544/60000]\n",
      "loss: 266.329163  [37184/60000]\n",
      "loss: 75.646263  [37824/60000]\n",
      "loss: 133.586121  [38464/60000]\n",
      "loss: 134.168610  [39104/60000]\n",
      "loss: 173.775085  [39744/60000]\n",
      "loss: 170.845901  [40384/60000]\n",
      "loss: 215.426315  [41024/60000]\n",
      "loss: 103.731194  [41664/60000]\n",
      "loss: 202.508484  [42304/60000]\n",
      "loss: 339.105072  [42944/60000]\n",
      "loss: 84.276367  [43584/60000]\n",
      "loss: 160.259399  [44224/60000]\n",
      "loss: 120.670593  [44864/60000]\n",
      "loss: 82.900574  [45504/60000]\n",
      "loss: 95.902672  [46144/60000]\n",
      "loss: 105.328232  [46784/60000]\n",
      "loss: 314.687256  [47424/60000]\n",
      "loss: 103.980072  [48064/60000]\n",
      "loss: 88.850052  [48704/60000]\n",
      "loss: 133.733719  [49344/60000]\n",
      "loss: 143.252792  [49984/60000]\n",
      "loss: 107.941071  [50624/60000]\n",
      "loss: 214.798035  [51264/60000]\n",
      "loss: 405.267395  [51904/60000]\n",
      "loss: 231.036194  [52544/60000]\n",
      "loss: 171.771591  [53184/60000]\n",
      "loss: 75.360344  [53824/60000]\n",
      "loss: 83.175529  [54464/60000]\n",
      "loss: 131.484039  [55104/60000]\n",
      "loss: 82.214127  [55744/60000]\n",
      "loss: 78.146935  [56384/60000]\n",
      "loss: 90.638542  [57024/60000]\n",
      "loss: 118.942398  [57664/60000]\n",
      "loss: 49.246918  [58304/60000]\n",
      "loss: 69.860680  [58944/60000]\n",
      "loss: 198.249756  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 221.125046 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 163.191971  [   64/60000]\n",
      "loss: 74.630760  [  704/60000]\n",
      "loss: 53.130325  [ 1344/60000]\n",
      "loss: 117.459503  [ 1984/60000]\n",
      "loss: 106.891312  [ 2624/60000]\n",
      "loss: 91.549332  [ 3264/60000]\n",
      "loss: 161.708603  [ 3904/60000]\n",
      "loss: 121.388626  [ 4544/60000]\n",
      "loss: 104.337769  [ 5184/60000]\n",
      "loss: 61.496861  [ 5824/60000]\n",
      "loss: 33.688702  [ 6464/60000]\n",
      "loss: 131.057236  [ 7104/60000]\n",
      "loss: 221.809158  [ 7744/60000]\n",
      "loss: 123.286858  [ 8384/60000]\n",
      "loss: 90.586700  [ 9024/60000]\n",
      "loss: 318.919983  [ 9664/60000]\n",
      "loss: 117.600418  [10304/60000]\n",
      "loss: 214.510666  [10944/60000]\n",
      "loss: 111.574097  [11584/60000]\n",
      "loss: 231.866028  [12224/60000]\n",
      "loss: 233.924393  [12864/60000]\n",
      "loss: 107.199150  [13504/60000]\n",
      "loss: 127.561096  [14144/60000]\n",
      "loss: 99.658997  [14784/60000]\n",
      "loss: 37.478554  [15424/60000]\n",
      "loss: 103.400490  [16064/60000]\n",
      "loss: 130.064682  [16704/60000]\n",
      "loss: 166.979462  [17344/60000]\n",
      "loss: 88.129425  [17984/60000]\n",
      "loss: 63.846882  [18624/60000]\n",
      "loss: 147.904022  [19264/60000]\n",
      "loss: 92.575043  [19904/60000]\n",
      "loss: 62.398201  [20544/60000]\n",
      "loss: 80.368759  [21184/60000]\n",
      "loss: 304.442139  [21824/60000]\n",
      "loss: 255.058762  [22464/60000]\n",
      "loss: 124.756210  [23104/60000]\n",
      "loss: 82.852966  [23744/60000]\n",
      "loss: 120.466644  [24384/60000]\n",
      "loss: 95.878502  [25024/60000]\n",
      "loss: 94.805794  [25664/60000]\n",
      "loss: 247.411713  [26304/60000]\n",
      "loss: 127.665009  [26944/60000]\n",
      "loss: 150.224426  [27584/60000]\n",
      "loss: 46.634117  [28224/60000]\n",
      "loss: 97.223984  [28864/60000]\n",
      "loss: 195.965027  [29504/60000]\n",
      "loss: 236.833054  [30144/60000]\n",
      "loss: 87.761703  [30784/60000]\n",
      "loss: 53.934624  [31424/60000]\n",
      "loss: 98.738052  [32064/60000]\n",
      "loss: 94.909286  [32704/60000]\n",
      "loss: 37.944782  [33344/60000]\n",
      "loss: 244.896881  [33984/60000]\n",
      "loss: 67.747932  [34624/60000]\n",
      "loss: 42.818909  [35264/60000]\n",
      "loss: 156.200775  [35904/60000]\n",
      "loss: 128.155411  [36544/60000]\n",
      "loss: 297.923187  [37184/60000]\n",
      "loss: 36.175827  [37824/60000]\n",
      "loss: 156.890106  [38464/60000]\n",
      "loss: 193.541016  [39104/60000]\n",
      "loss: 119.715096  [39744/60000]\n",
      "loss: 112.036011  [40384/60000]\n",
      "loss: 167.637756  [41024/60000]\n",
      "loss: 100.292435  [41664/60000]\n",
      "loss: 158.319092  [42304/60000]\n",
      "loss: 57.678783  [42944/60000]\n",
      "loss: 74.744209  [43584/60000]\n",
      "loss: 94.267838  [44224/60000]\n",
      "loss: 76.090813  [44864/60000]\n",
      "loss: 80.498116  [45504/60000]\n",
      "loss: 80.223808  [46144/60000]\n",
      "loss: 157.708893  [46784/60000]\n",
      "loss: 178.353821  [47424/60000]\n",
      "loss: 131.993134  [48064/60000]\n",
      "loss: 117.148895  [48704/60000]\n",
      "loss: 82.739594  [49344/60000]\n",
      "loss: 137.162216  [49984/60000]\n",
      "loss: 70.257812  [50624/60000]\n",
      "loss: 130.742371  [51264/60000]\n",
      "loss: 194.249069  [51904/60000]\n",
      "loss: 351.715027  [52544/60000]\n",
      "loss: 153.645660  [53184/60000]\n",
      "loss: 106.261429  [53824/60000]\n",
      "loss: 82.007027  [54464/60000]\n",
      "loss: 107.898842  [55104/60000]\n",
      "loss: 65.289238  [55744/60000]\n",
      "loss: 44.433502  [56384/60000]\n",
      "loss: 119.510620  [57024/60000]\n",
      "loss: 97.429665  [57664/60000]\n",
      "loss: 46.244949  [58304/60000]\n",
      "loss: 56.753845  [58944/60000]\n",
      "loss: 215.389099  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 174.046332 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 101.943497  [   64/60000]\n",
      "loss: 92.679306  [  704/60000]\n",
      "loss: 62.403988  [ 1344/60000]\n",
      "loss: 156.275604  [ 1984/60000]\n",
      "loss: 141.447861  [ 2624/60000]\n",
      "loss: 93.716560  [ 3264/60000]\n",
      "loss: 123.129822  [ 3904/60000]\n",
      "loss: 63.931206  [ 4544/60000]\n",
      "loss: 269.726685  [ 5184/60000]\n",
      "loss: 56.974903  [ 5824/60000]\n",
      "loss: 42.605331  [ 6464/60000]\n",
      "loss: 244.852798  [ 7104/60000]\n",
      "loss: 214.458557  [ 7744/60000]\n",
      "loss: 154.417023  [ 8384/60000]\n",
      "loss: 124.851868  [ 9024/60000]\n",
      "loss: 126.184189  [ 9664/60000]\n",
      "loss: 113.282074  [10304/60000]\n",
      "loss: 87.853180  [10944/60000]\n",
      "loss: 131.773956  [11584/60000]\n",
      "loss: 247.309052  [12224/60000]\n",
      "loss: 273.635193  [12864/60000]\n",
      "loss: 96.554565  [13504/60000]\n",
      "loss: 41.278091  [14144/60000]\n",
      "loss: 100.546806  [14784/60000]\n",
      "loss: 50.252419  [15424/60000]\n",
      "loss: 193.004730  [16064/60000]\n",
      "loss: 119.300423  [16704/60000]\n",
      "loss: 142.355103  [17344/60000]\n",
      "loss: 109.743347  [17984/60000]\n",
      "loss: 79.131165  [18624/60000]\n",
      "loss: 140.559143  [19264/60000]\n",
      "loss: 137.253647  [19904/60000]\n",
      "loss: 51.494736  [20544/60000]\n",
      "loss: 75.280678  [21184/60000]\n",
      "loss: 207.826233  [21824/60000]\n",
      "loss: 222.031601  [22464/60000]\n",
      "loss: 164.935822  [23104/60000]\n",
      "loss: 80.754196  [23744/60000]\n",
      "loss: 271.327484  [24384/60000]\n",
      "loss: 79.114716  [25024/60000]\n",
      "loss: 93.128677  [25664/60000]\n",
      "loss: 173.562775  [26304/60000]\n",
      "loss: 143.300781  [26944/60000]\n",
      "loss: 205.839691  [27584/60000]\n",
      "loss: 51.048199  [28224/60000]\n",
      "loss: 121.704025  [28864/60000]\n",
      "loss: 125.569901  [29504/60000]\n",
      "loss: 235.666412  [30144/60000]\n",
      "loss: 106.153915  [30784/60000]\n",
      "loss: 42.299622  [31424/60000]\n",
      "loss: 135.336182  [32064/60000]\n",
      "loss: 146.483124  [32704/60000]\n",
      "loss: 87.974510  [33344/60000]\n",
      "loss: 138.631348  [33984/60000]\n",
      "loss: 45.612854  [34624/60000]\n",
      "loss: 51.941418  [35264/60000]\n",
      "loss: 128.079727  [35904/60000]\n",
      "loss: 139.888702  [36544/60000]\n",
      "loss: 126.544136  [37184/60000]\n",
      "loss: 42.662216  [37824/60000]\n",
      "loss: 104.681641  [38464/60000]\n",
      "loss: 108.941948  [39104/60000]\n",
      "loss: 122.202423  [39744/60000]\n",
      "loss: 117.122894  [40384/60000]\n",
      "loss: 125.482079  [41024/60000]\n",
      "loss: 110.481430  [41664/60000]\n",
      "loss: 126.750687  [42304/60000]\n",
      "loss: 289.891357  [42944/60000]\n",
      "loss: 228.058685  [43584/60000]\n",
      "loss: 203.963043  [44224/60000]\n",
      "loss: 88.360428  [44864/60000]\n",
      "loss: 73.031403  [45504/60000]\n",
      "loss: 69.166550  [46144/60000]\n",
      "loss: 148.752731  [46784/60000]\n",
      "loss: 149.693970  [47424/60000]\n",
      "loss: 132.243774  [48064/60000]\n",
      "loss: 96.262177  [48704/60000]\n",
      "loss: 72.734138  [49344/60000]\n",
      "loss: 120.400383  [49984/60000]\n",
      "loss: 56.199081  [50624/60000]\n",
      "loss: 142.085815  [51264/60000]\n",
      "loss: 305.781616  [51904/60000]\n",
      "loss: 317.922363  [52544/60000]\n",
      "loss: 74.150887  [53184/60000]\n",
      "loss: 54.485779  [53824/60000]\n",
      "loss: 109.187996  [54464/60000]\n",
      "loss: 79.013214  [55104/60000]\n",
      "loss: 75.277145  [55744/60000]\n",
      "loss: 131.783615  [56384/60000]\n",
      "loss: 181.414337  [57024/60000]\n",
      "loss: 87.586807  [57664/60000]\n",
      "loss: 57.674870  [58304/60000]\n",
      "loss: 94.257515  [58944/60000]\n",
      "loss: 161.202408  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 186.868370 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 136.431976  [   64/60000]\n",
      "loss: 113.347336  [  704/60000]\n",
      "loss: 45.490097  [ 1344/60000]\n",
      "loss: 61.468647  [ 1984/60000]\n",
      "loss: 108.801697  [ 2624/60000]\n",
      "loss: 83.003136  [ 3264/60000]\n",
      "loss: 137.414948  [ 3904/60000]\n",
      "loss: 47.750324  [ 4544/60000]\n",
      "loss: 136.860489  [ 5184/60000]\n",
      "loss: 116.503166  [ 5824/60000]\n",
      "loss: 61.957512  [ 6464/60000]\n",
      "loss: 91.253319  [ 7104/60000]\n",
      "loss: 286.949615  [ 7744/60000]\n",
      "loss: 145.712189  [ 8384/60000]\n",
      "loss: 98.008209  [ 9024/60000]\n",
      "loss: 62.936245  [ 9664/60000]\n",
      "loss: 154.875580  [10304/60000]\n",
      "loss: 238.309891  [10944/60000]\n",
      "loss: 110.489883  [11584/60000]\n",
      "loss: 174.784454  [12224/60000]\n",
      "loss: 113.748856  [12864/60000]\n",
      "loss: 155.416504  [13504/60000]\n",
      "loss: 58.169724  [14144/60000]\n",
      "loss: 90.692345  [14784/60000]\n",
      "loss: 45.666683  [15424/60000]\n",
      "loss: 92.769608  [16064/60000]\n",
      "loss: 60.290634  [16704/60000]\n",
      "loss: 185.128098  [17344/60000]\n",
      "loss: 153.312134  [17984/60000]\n",
      "loss: 78.745186  [18624/60000]\n",
      "loss: 147.011597  [19264/60000]\n",
      "loss: 110.841896  [19904/60000]\n",
      "loss: 44.727867  [20544/60000]\n",
      "loss: 75.494972  [21184/60000]\n",
      "loss: 292.436798  [21824/60000]\n",
      "loss: 90.772758  [22464/60000]\n",
      "loss: 124.193619  [23104/60000]\n",
      "loss: 71.062668  [23744/60000]\n",
      "loss: 76.879982  [24384/60000]\n",
      "loss: 73.572159  [25024/60000]\n",
      "loss: 109.411850  [25664/60000]\n",
      "loss: 230.726486  [26304/60000]\n",
      "loss: 97.976990  [26944/60000]\n",
      "loss: 201.652756  [27584/60000]\n",
      "loss: 55.663261  [28224/60000]\n",
      "loss: 186.922623  [28864/60000]\n",
      "loss: 104.125397  [29504/60000]\n",
      "loss: 147.549774  [30144/60000]\n",
      "loss: 98.533234  [30784/60000]\n",
      "loss: 51.206253  [31424/60000]\n",
      "loss: 155.997437  [32064/60000]\n",
      "loss: 94.530457  [32704/60000]\n",
      "loss: 69.137451  [33344/60000]\n",
      "loss: 212.373032  [33984/60000]\n",
      "loss: 44.631516  [34624/60000]\n",
      "loss: 76.465439  [35264/60000]\n",
      "loss: 84.575584  [35904/60000]\n",
      "loss: 112.149979  [36544/60000]\n",
      "loss: 271.196472  [37184/60000]\n",
      "loss: 42.947002  [37824/60000]\n",
      "loss: 150.487137  [38464/60000]\n",
      "loss: 150.205032  [39104/60000]\n",
      "loss: 98.551819  [39744/60000]\n",
      "loss: 134.815033  [40384/60000]\n",
      "loss: 153.195938  [41024/60000]\n",
      "loss: 102.375824  [41664/60000]\n",
      "loss: 143.310791  [42304/60000]\n",
      "loss: 108.805595  [42944/60000]\n",
      "loss: 71.366821  [43584/60000]\n",
      "loss: 135.487518  [44224/60000]\n",
      "loss: 120.187401  [44864/60000]\n",
      "loss: 81.586914  [45504/60000]\n",
      "loss: 92.965279  [46144/60000]\n",
      "loss: 121.879395  [46784/60000]\n",
      "loss: 153.712708  [47424/60000]\n",
      "loss: 134.923401  [48064/60000]\n",
      "loss: 100.802422  [48704/60000]\n",
      "loss: 79.052902  [49344/60000]\n",
      "loss: 110.107254  [49984/60000]\n",
      "loss: 64.946915  [50624/60000]\n",
      "loss: 198.553650  [51264/60000]\n",
      "loss: 361.671387  [51904/60000]\n",
      "loss: 411.345215  [52544/60000]\n",
      "loss: 189.312286  [53184/60000]\n",
      "loss: 51.648842  [53824/60000]\n",
      "loss: 128.635498  [54464/60000]\n",
      "loss: 77.077316  [55104/60000]\n",
      "loss: 68.333221  [55744/60000]\n",
      "loss: 66.268784  [56384/60000]\n",
      "loss: 90.483269  [57024/60000]\n",
      "loss: 171.997864  [57664/60000]\n",
      "loss: 50.485039  [58304/60000]\n",
      "loss: 71.516861  [58944/60000]\n",
      "loss: 212.276382  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 174.425391 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 106.073120  [   64/60000]\n",
      "loss: 109.477524  [  704/60000]\n",
      "loss: 48.023811  [ 1344/60000]\n",
      "loss: 100.822899  [ 1984/60000]\n",
      "loss: 102.826393  [ 2624/60000]\n",
      "loss: 94.508530  [ 3264/60000]\n",
      "loss: 135.427628  [ 3904/60000]\n",
      "loss: 59.337555  [ 4544/60000]\n",
      "loss: 155.699234  [ 5184/60000]\n",
      "loss: 136.399216  [ 5824/60000]\n",
      "loss: 61.114136  [ 6464/60000]\n",
      "loss: 76.915466  [ 7104/60000]\n",
      "loss: 223.467880  [ 7744/60000]\n",
      "loss: 141.859711  [ 8384/60000]\n",
      "loss: 88.110001  [ 9024/60000]\n",
      "loss: 104.782532  [ 9664/60000]\n",
      "loss: 64.629463  [10304/60000]\n",
      "loss: 63.525124  [10944/60000]\n",
      "loss: 105.388145  [11584/60000]\n",
      "loss: 458.259888  [12224/60000]\n",
      "loss: 198.205505  [12864/60000]\n",
      "loss: 159.300766  [13504/60000]\n",
      "loss: 147.956192  [14144/60000]\n",
      "loss: 128.163940  [14784/60000]\n",
      "loss: 63.129379  [15424/60000]\n",
      "loss: 105.749496  [16064/60000]\n",
      "loss: 146.327881  [16704/60000]\n",
      "loss: 192.683807  [17344/60000]\n",
      "loss: 117.954071  [17984/60000]\n",
      "loss: 82.806618  [18624/60000]\n",
      "loss: 117.209320  [19264/60000]\n",
      "loss: 115.394783  [19904/60000]\n",
      "loss: 56.777546  [20544/60000]\n",
      "loss: 110.505287  [21184/60000]\n",
      "loss: 366.600372  [21824/60000]\n",
      "loss: 71.620651  [22464/60000]\n",
      "loss: 154.521530  [23104/60000]\n",
      "loss: 75.814468  [23744/60000]\n",
      "loss: 122.290482  [24384/60000]\n",
      "loss: 68.470963  [25024/60000]\n",
      "loss: 90.028900  [25664/60000]\n",
      "loss: 155.360764  [26304/60000]\n",
      "loss: 129.079422  [26944/60000]\n",
      "loss: 158.894470  [27584/60000]\n",
      "loss: 60.495312  [28224/60000]\n",
      "loss: 31.959076  [28864/60000]\n",
      "loss: 150.507843  [29504/60000]\n",
      "loss: 211.238800  [30144/60000]\n",
      "loss: 75.474396  [30784/60000]\n",
      "loss: 38.842804  [31424/60000]\n",
      "loss: 85.535469  [32064/60000]\n",
      "loss: 93.434372  [32704/60000]\n",
      "loss: 71.910004  [33344/60000]\n",
      "loss: 165.189026  [33984/60000]\n",
      "loss: 43.850086  [34624/60000]\n",
      "loss: 47.773609  [35264/60000]\n",
      "loss: 81.271217  [35904/60000]\n",
      "loss: 102.891609  [36544/60000]\n",
      "loss: 256.422363  [37184/60000]\n",
      "loss: 75.116020  [37824/60000]\n",
      "loss: 149.503510  [38464/60000]\n",
      "loss: 132.936676  [39104/60000]\n",
      "loss: 119.640610  [39744/60000]\n",
      "loss: 93.819527  [40384/60000]\n",
      "loss: 126.971947  [41024/60000]\n",
      "loss: 108.358360  [41664/60000]\n",
      "loss: 115.524887  [42304/60000]\n",
      "loss: 174.736420  [42944/60000]\n",
      "loss: 271.016602  [43584/60000]\n",
      "loss: 247.371231  [44224/60000]\n",
      "loss: 122.249535  [44864/60000]\n",
      "loss: 90.708237  [45504/60000]\n",
      "loss: 84.078979  [46144/60000]\n",
      "loss: 126.129471  [46784/60000]\n",
      "loss: 132.657364  [47424/60000]\n",
      "loss: 116.920341  [48064/60000]\n",
      "loss: 127.188408  [48704/60000]\n",
      "loss: 89.201927  [49344/60000]\n",
      "loss: 141.029785  [49984/60000]\n",
      "loss: 50.208679  [50624/60000]\n",
      "loss: 218.194672  [51264/60000]\n",
      "loss: 376.629242  [51904/60000]\n",
      "loss: 494.239014  [52544/60000]\n",
      "loss: 237.137482  [53184/60000]\n",
      "loss: 57.137123  [53824/60000]\n",
      "loss: 163.086868  [54464/60000]\n",
      "loss: 70.301422  [55104/60000]\n",
      "loss: 79.484848  [55744/60000]\n",
      "loss: 93.393814  [56384/60000]\n",
      "loss: 112.174835  [57024/60000]\n",
      "loss: 138.249542  [57664/60000]\n",
      "loss: 118.208580  [58304/60000]\n",
      "loss: 51.621773  [58944/60000]\n",
      "loss: 206.002960  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.1%, Avg loss: 194.882301 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 130.537674  [   64/60000]\n",
      "loss: 70.602585  [  704/60000]\n",
      "loss: 49.594021  [ 1344/60000]\n",
      "loss: 96.490028  [ 1984/60000]\n",
      "loss: 100.998955  [ 2624/60000]\n",
      "loss: 97.599571  [ 3264/60000]\n",
      "loss: 124.098160  [ 3904/60000]\n",
      "loss: 57.219231  [ 4544/60000]\n",
      "loss: 170.806122  [ 5184/60000]\n",
      "loss: 138.484909  [ 5824/60000]\n",
      "loss: 59.740463  [ 6464/60000]\n",
      "loss: 107.338364  [ 7104/60000]\n",
      "loss: 256.731171  [ 7744/60000]\n",
      "loss: 323.988953  [ 8384/60000]\n",
      "loss: 102.571426  [ 9024/60000]\n",
      "loss: 72.950142  [ 9664/60000]\n",
      "loss: 67.773361  [10304/60000]\n",
      "loss: 124.234535  [10944/60000]\n",
      "loss: 101.803612  [11584/60000]\n",
      "loss: 194.321106  [12224/60000]\n",
      "loss: 79.277420  [12864/60000]\n",
      "loss: 147.564819  [13504/60000]\n",
      "loss: 96.565132  [14144/60000]\n",
      "loss: 97.443550  [14784/60000]\n",
      "loss: 31.572166  [15424/60000]\n",
      "loss: 169.720184  [16064/60000]\n",
      "loss: 122.073601  [16704/60000]\n",
      "loss: 130.829041  [17344/60000]\n",
      "loss: 99.025520  [17984/60000]\n",
      "loss: 53.004730  [18624/60000]\n",
      "loss: 152.134781  [19264/60000]\n",
      "loss: 200.551422  [19904/60000]\n",
      "loss: 56.451714  [20544/60000]\n",
      "loss: 62.451832  [21184/60000]\n",
      "loss: 302.992493  [21824/60000]\n",
      "loss: 90.808685  [22464/60000]\n",
      "loss: 98.002098  [23104/60000]\n",
      "loss: 80.039558  [23744/60000]\n",
      "loss: 123.740494  [24384/60000]\n",
      "loss: 79.047043  [25024/60000]\n",
      "loss: 178.378296  [25664/60000]\n",
      "loss: 154.603668  [26304/60000]\n",
      "loss: 89.033615  [26944/60000]\n",
      "loss: 224.253876  [27584/60000]\n",
      "loss: 101.825760  [28224/60000]\n",
      "loss: 29.881310  [28864/60000]\n",
      "loss: 191.932281  [29504/60000]\n",
      "loss: 179.397186  [30144/60000]\n",
      "loss: 109.994003  [30784/60000]\n",
      "loss: 23.027122  [31424/60000]\n",
      "loss: 82.249809  [32064/60000]\n",
      "loss: 90.686165  [32704/60000]\n",
      "loss: 33.425213  [33344/60000]\n",
      "loss: 153.806305  [33984/60000]\n",
      "loss: 58.232895  [34624/60000]\n",
      "loss: 64.440521  [35264/60000]\n",
      "loss: 64.795258  [35904/60000]\n",
      "loss: 78.157837  [36544/60000]\n",
      "loss: 268.614990  [37184/60000]\n",
      "loss: 34.312069  [37824/60000]\n",
      "loss: 107.517609  [38464/60000]\n",
      "loss: 58.428417  [39104/60000]\n",
      "loss: 91.806664  [39744/60000]\n",
      "loss: 162.412354  [40384/60000]\n",
      "loss: 68.115181  [41024/60000]\n",
      "loss: 121.239548  [41664/60000]\n",
      "loss: 183.590302  [42304/60000]\n",
      "loss: 319.912994  [42944/60000]\n",
      "loss: 187.348083  [43584/60000]\n",
      "loss: 154.750793  [44224/60000]\n",
      "loss: 82.806671  [44864/60000]\n",
      "loss: 124.832787  [45504/60000]\n",
      "loss: 83.780327  [46144/60000]\n",
      "loss: 148.153244  [46784/60000]\n",
      "loss: 71.240082  [47424/60000]\n",
      "loss: 108.383636  [48064/60000]\n",
      "loss: 82.828972  [48704/60000]\n",
      "loss: 72.131638  [49344/60000]\n",
      "loss: 121.988098  [49984/60000]\n",
      "loss: 54.377724  [50624/60000]\n",
      "loss: 162.812241  [51264/60000]\n",
      "loss: 271.277832  [51904/60000]\n",
      "loss: 249.785278  [52544/60000]\n",
      "loss: 207.401459  [53184/60000]\n",
      "loss: 85.935005  [53824/60000]\n",
      "loss: 144.255798  [54464/60000]\n",
      "loss: 118.089485  [55104/60000]\n",
      "loss: 73.739876  [55744/60000]\n",
      "loss: 43.488491  [56384/60000]\n",
      "loss: 101.062119  [57024/60000]\n",
      "loss: 129.673126  [57664/60000]\n",
      "loss: 50.050278  [58304/60000]\n",
      "loss: 67.287277  [58944/60000]\n",
      "loss: 444.731659  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 312.874500 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 252.609161  [   64/60000]\n",
      "loss: 56.850929  [  704/60000]\n",
      "loss: 81.765533  [ 1344/60000]\n",
      "loss: 71.812447  [ 1984/60000]\n",
      "loss: 75.095306  [ 2624/60000]\n",
      "loss: 94.931190  [ 3264/60000]\n",
      "loss: 117.554787  [ 3904/60000]\n",
      "loss: 59.882370  [ 4544/60000]\n",
      "loss: 249.477631  [ 5184/60000]\n",
      "loss: 129.736710  [ 5824/60000]\n",
      "loss: 38.726952  [ 6464/60000]\n",
      "loss: 132.746536  [ 7104/60000]\n",
      "loss: 262.701843  [ 7744/60000]\n",
      "loss: 280.066895  [ 8384/60000]\n",
      "loss: 110.559448  [ 9024/60000]\n",
      "loss: 295.897186  [ 9664/60000]\n",
      "loss: 125.723083  [10304/60000]\n",
      "loss: 68.155106  [10944/60000]\n",
      "loss: 107.807083  [11584/60000]\n",
      "loss: 231.714676  [12224/60000]\n",
      "loss: 399.369873  [12864/60000]\n",
      "loss: 162.031067  [13504/60000]\n",
      "loss: 41.935047  [14144/60000]\n",
      "loss: 119.699234  [14784/60000]\n",
      "loss: 51.913025  [15424/60000]\n",
      "loss: 99.941200  [16064/60000]\n",
      "loss: 109.472763  [16704/60000]\n",
      "loss: 129.702148  [17344/60000]\n",
      "loss: 100.069016  [17984/60000]\n",
      "loss: 60.756744  [18624/60000]\n",
      "loss: 116.929932  [19264/60000]\n",
      "loss: 178.517334  [19904/60000]\n",
      "loss: 67.810669  [20544/60000]\n",
      "loss: 81.801376  [21184/60000]\n",
      "loss: 328.340729  [21824/60000]\n",
      "loss: 105.839302  [22464/60000]\n",
      "loss: 142.705566  [23104/60000]\n",
      "loss: 73.653694  [23744/60000]\n",
      "loss: 244.404861  [24384/60000]\n",
      "loss: 87.434303  [25024/60000]\n",
      "loss: 138.061340  [25664/60000]\n",
      "loss: 232.167908  [26304/60000]\n",
      "loss: 98.374001  [26944/60000]\n",
      "loss: 197.211853  [27584/60000]\n",
      "loss: 54.971390  [28224/60000]\n",
      "loss: 113.342812  [28864/60000]\n",
      "loss: 133.233261  [29504/60000]\n",
      "loss: 154.495758  [30144/60000]\n",
      "loss: 98.799400  [30784/60000]\n",
      "loss: 55.901096  [31424/60000]\n",
      "loss: 85.543976  [32064/60000]\n",
      "loss: 70.887947  [32704/60000]\n",
      "loss: 46.560257  [33344/60000]\n",
      "loss: 224.401672  [33984/60000]\n",
      "loss: 73.331474  [34624/60000]\n",
      "loss: 41.965988  [35264/60000]\n",
      "loss: 68.321999  [35904/60000]\n",
      "loss: 75.398140  [36544/60000]\n",
      "loss: 282.269165  [37184/60000]\n",
      "loss: 81.130341  [37824/60000]\n",
      "loss: 139.849594  [38464/60000]\n",
      "loss: 84.485039  [39104/60000]\n",
      "loss: 132.130920  [39744/60000]\n",
      "loss: 142.529068  [40384/60000]\n",
      "loss: 224.542084  [41024/60000]\n",
      "loss: 103.293869  [41664/60000]\n",
      "loss: 187.133118  [42304/60000]\n",
      "loss: 111.206612  [42944/60000]\n",
      "loss: 52.544518  [43584/60000]\n",
      "loss: 163.190659  [44224/60000]\n",
      "loss: 162.894379  [44864/60000]\n",
      "loss: 113.953323  [45504/60000]\n",
      "loss: 88.122910  [46144/60000]\n",
      "loss: 126.794563  [46784/60000]\n",
      "loss: 203.374115  [47424/60000]\n",
      "loss: 121.229324  [48064/60000]\n",
      "loss: 120.647278  [48704/60000]\n",
      "loss: 101.568527  [49344/60000]\n",
      "loss: 107.007057  [49984/60000]\n",
      "loss: 69.228897  [50624/60000]\n",
      "loss: 179.095840  [51264/60000]\n",
      "loss: 316.665466  [51904/60000]\n",
      "loss: 396.400665  [52544/60000]\n",
      "loss: 254.913239  [53184/60000]\n",
      "loss: 67.455643  [53824/60000]\n",
      "loss: 99.566719  [54464/60000]\n",
      "loss: 75.304344  [55104/60000]\n",
      "loss: 66.952927  [55744/60000]\n",
      "loss: 48.726788  [56384/60000]\n",
      "loss: 83.556007  [57024/60000]\n",
      "loss: 129.102737  [57664/60000]\n",
      "loss: 33.919426  [58304/60000]\n",
      "loss: 68.589584  [58944/60000]\n",
      "loss: 267.124176  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.0%, Avg loss: 246.550214 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 189.527588  [   64/60000]\n",
      "loss: 99.058899  [  704/60000]\n",
      "loss: 52.171623  [ 1344/60000]\n",
      "loss: 66.662674  [ 1984/60000]\n",
      "loss: 107.854782  [ 2624/60000]\n",
      "loss: 77.103981  [ 3264/60000]\n",
      "loss: 127.589371  [ 3904/60000]\n",
      "loss: 63.896633  [ 4544/60000]\n",
      "loss: 181.511978  [ 5184/60000]\n",
      "loss: 36.315308  [ 5824/60000]\n",
      "loss: 48.196758  [ 6464/60000]\n",
      "loss: 177.526215  [ 7104/60000]\n",
      "loss: 172.874115  [ 7744/60000]\n",
      "loss: 115.401672  [ 8384/60000]\n",
      "loss: 86.631607  [ 9024/60000]\n",
      "loss: 50.505547  [ 9664/60000]\n",
      "loss: 154.531082  [10304/60000]\n",
      "loss: 192.436417  [10944/60000]\n",
      "loss: 84.898216  [11584/60000]\n",
      "loss: 523.093567  [12224/60000]\n",
      "loss: 64.755486  [12864/60000]\n",
      "loss: 105.092010  [13504/60000]\n",
      "loss: 33.127747  [14144/60000]\n",
      "loss: 71.204651  [14784/60000]\n",
      "loss: 49.310776  [15424/60000]\n",
      "loss: 162.938095  [16064/60000]\n",
      "loss: 132.343170  [16704/60000]\n",
      "loss: 162.470154  [17344/60000]\n",
      "loss: 84.374405  [17984/60000]\n",
      "loss: 116.880585  [18624/60000]\n",
      "loss: 151.281616  [19264/60000]\n",
      "loss: 107.804955  [19904/60000]\n",
      "loss: 46.732613  [20544/60000]\n",
      "loss: 92.026810  [21184/60000]\n",
      "loss: 290.772278  [21824/60000]\n",
      "loss: 50.673832  [22464/60000]\n",
      "loss: 198.016724  [23104/60000]\n",
      "loss: 69.778755  [23744/60000]\n",
      "loss: 194.563217  [24384/60000]\n",
      "loss: 62.792564  [25024/60000]\n",
      "loss: 90.808891  [25664/60000]\n",
      "loss: 181.166000  [26304/60000]\n",
      "loss: 131.853882  [26944/60000]\n",
      "loss: 191.362610  [27584/60000]\n",
      "loss: 52.715042  [28224/60000]\n",
      "loss: 41.744968  [28864/60000]\n",
      "loss: 144.004211  [29504/60000]\n",
      "loss: 142.609741  [30144/60000]\n",
      "loss: 108.365143  [30784/60000]\n",
      "loss: 50.088692  [31424/60000]\n",
      "loss: 103.378448  [32064/60000]\n",
      "loss: 146.214630  [32704/60000]\n",
      "loss: 58.666912  [33344/60000]\n",
      "loss: 149.469467  [33984/60000]\n",
      "loss: 57.035866  [34624/60000]\n",
      "loss: 44.617336  [35264/60000]\n",
      "loss: 81.290375  [35904/60000]\n",
      "loss: 193.545319  [36544/60000]\n",
      "loss: 119.439995  [37184/60000]\n",
      "loss: 72.082664  [37824/60000]\n",
      "loss: 139.874115  [38464/60000]\n",
      "loss: 103.293076  [39104/60000]\n",
      "loss: 93.561920  [39744/60000]\n",
      "loss: 71.219139  [40384/60000]\n",
      "loss: 139.414856  [41024/60000]\n",
      "loss: 88.282333  [41664/60000]\n",
      "loss: 177.214294  [42304/60000]\n",
      "loss: 153.725998  [42944/60000]\n",
      "loss: 72.314194  [43584/60000]\n",
      "loss: 250.160233  [44224/60000]\n",
      "loss: 159.853714  [44864/60000]\n",
      "loss: 85.392204  [45504/60000]\n",
      "loss: 107.599258  [46144/60000]\n",
      "loss: 162.911377  [46784/60000]\n",
      "loss: 67.914688  [47424/60000]\n",
      "loss: 106.869690  [48064/60000]\n",
      "loss: 99.052246  [48704/60000]\n",
      "loss: 72.683723  [49344/60000]\n",
      "loss: 128.027649  [49984/60000]\n",
      "loss: 98.523666  [50624/60000]\n",
      "loss: 245.695129  [51264/60000]\n",
      "loss: 303.897827  [51904/60000]\n",
      "loss: 235.164108  [52544/60000]\n",
      "loss: 99.054855  [53184/60000]\n",
      "loss: 79.040558  [53824/60000]\n",
      "loss: 177.464828  [54464/60000]\n",
      "loss: 77.717079  [55104/60000]\n",
      "loss: 70.207001  [55744/60000]\n",
      "loss: 88.104507  [56384/60000]\n",
      "loss: 107.299385  [57024/60000]\n",
      "loss: 113.079964  [57664/60000]\n",
      "loss: 28.802681  [58304/60000]\n",
      "loss: 63.928406  [58944/60000]\n",
      "loss: 255.447250  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 334.251862 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 263.788788  [   64/60000]\n",
      "loss: 81.347488  [  704/60000]\n",
      "loss: 31.336374  [ 1344/60000]\n",
      "loss: 184.340103  [ 1984/60000]\n",
      "loss: 131.965683  [ 2624/60000]\n",
      "loss: 99.095215  [ 3264/60000]\n",
      "loss: 228.541855  [ 3904/60000]\n",
      "loss: 48.424854  [ 4544/60000]\n",
      "loss: 354.891663  [ 5184/60000]\n",
      "loss: 38.306793  [ 5824/60000]\n",
      "loss: 59.494118  [ 6464/60000]\n",
      "loss: 84.847046  [ 7104/60000]\n",
      "loss: 203.699097  [ 7744/60000]\n",
      "loss: 91.719772  [ 8384/60000]\n",
      "loss: 139.072144  [ 9024/60000]\n",
      "loss: 290.480835  [ 9664/60000]\n",
      "loss: 80.497787  [10304/60000]\n",
      "loss: 68.486465  [10944/60000]\n",
      "loss: 174.154526  [11584/60000]\n",
      "loss: 173.575073  [12224/60000]\n",
      "loss: 301.808563  [12864/60000]\n",
      "loss: 144.401962  [13504/60000]\n",
      "loss: 65.052856  [14144/60000]\n",
      "loss: 85.695953  [14784/60000]\n",
      "loss: 16.271393  [15424/60000]\n",
      "loss: 83.205292  [16064/60000]\n",
      "loss: 79.016045  [16704/60000]\n",
      "loss: 139.970367  [17344/60000]\n",
      "loss: 70.865738  [17984/60000]\n",
      "loss: 84.505692  [18624/60000]\n",
      "loss: 168.918045  [19264/60000]\n",
      "loss: 141.047089  [19904/60000]\n",
      "loss: 50.840935  [20544/60000]\n",
      "loss: 71.979141  [21184/60000]\n",
      "loss: 271.121765  [21824/60000]\n",
      "loss: 95.773308  [22464/60000]\n",
      "loss: 254.483551  [23104/60000]\n",
      "loss: 65.811935  [23744/60000]\n",
      "loss: 212.234253  [24384/60000]\n",
      "loss: 69.398636  [25024/60000]\n",
      "loss: 103.214958  [25664/60000]\n",
      "loss: 140.516602  [26304/60000]\n",
      "loss: 115.170204  [26944/60000]\n",
      "loss: 161.183838  [27584/60000]\n",
      "loss: 54.560890  [28224/60000]\n",
      "loss: 153.696442  [28864/60000]\n",
      "loss: 230.702301  [29504/60000]\n",
      "loss: 199.837006  [30144/60000]\n",
      "loss: 136.412552  [30784/60000]\n",
      "loss: 39.070698  [31424/60000]\n",
      "loss: 100.114799  [32064/60000]\n",
      "loss: 83.423393  [32704/60000]\n",
      "loss: 47.664143  [33344/60000]\n",
      "loss: 139.993347  [33984/60000]\n",
      "loss: 42.130547  [34624/60000]\n",
      "loss: 43.064007  [35264/60000]\n",
      "loss: 87.037872  [35904/60000]\n",
      "loss: 78.114975  [36544/60000]\n",
      "loss: 248.938660  [37184/60000]\n",
      "loss: 49.058365  [37824/60000]\n",
      "loss: 101.391594  [38464/60000]\n",
      "loss: 66.838326  [39104/60000]\n",
      "loss: 180.911362  [39744/60000]\n",
      "loss: 106.383705  [40384/60000]\n",
      "loss: 110.333633  [41024/60000]\n",
      "loss: 139.543747  [41664/60000]\n",
      "loss: 201.738861  [42304/60000]\n",
      "loss: 76.699646  [42944/60000]\n",
      "loss: 76.225555  [43584/60000]\n",
      "loss: 186.113892  [44224/60000]\n",
      "loss: 87.122574  [44864/60000]\n",
      "loss: 98.053146  [45504/60000]\n",
      "loss: 81.786476  [46144/60000]\n",
      "loss: 104.576202  [46784/60000]\n",
      "loss: 36.413311  [47424/60000]\n",
      "loss: 103.278381  [48064/60000]\n",
      "loss: 135.595734  [48704/60000]\n",
      "loss: 67.452202  [49344/60000]\n",
      "loss: 157.815735  [49984/60000]\n",
      "loss: 64.997650  [50624/60000]\n",
      "loss: 127.586357  [51264/60000]\n",
      "loss: 202.346039  [51904/60000]\n",
      "loss: 418.384186  [52544/60000]\n",
      "loss: 55.062065  [53184/60000]\n",
      "loss: 115.253067  [53824/60000]\n",
      "loss: 59.078766  [54464/60000]\n",
      "loss: 55.378525  [55104/60000]\n",
      "loss: 69.111984  [55744/60000]\n",
      "loss: 57.846653  [56384/60000]\n",
      "loss: 68.352654  [57024/60000]\n",
      "loss: 108.194145  [57664/60000]\n",
      "loss: 32.137772  [58304/60000]\n",
      "loss: 66.330215  [58944/60000]\n",
      "loss: 241.189514  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 262.066939 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 199.792023  [   64/60000]\n",
      "loss: 133.233734  [  704/60000]\n",
      "loss: 29.234531  [ 1344/60000]\n",
      "loss: 114.824539  [ 1984/60000]\n",
      "loss: 109.374611  [ 2624/60000]\n",
      "loss: 92.502327  [ 3264/60000]\n",
      "loss: 118.003159  [ 3904/60000]\n",
      "loss: 61.045620  [ 4544/60000]\n",
      "loss: 94.479927  [ 5184/60000]\n",
      "loss: 99.959610  [ 5824/60000]\n",
      "loss: 36.255558  [ 6464/60000]\n",
      "loss: 86.212219  [ 7104/60000]\n",
      "loss: 172.043762  [ 7744/60000]\n",
      "loss: 111.911392  [ 8384/60000]\n",
      "loss: 84.572777  [ 9024/60000]\n",
      "loss: 111.705368  [ 9664/60000]\n",
      "loss: 109.034790  [10304/60000]\n",
      "loss: 104.153564  [10944/60000]\n",
      "loss: 112.636765  [11584/60000]\n",
      "loss: 241.328888  [12224/60000]\n",
      "loss: 114.543869  [12864/60000]\n",
      "loss: 129.861053  [13504/60000]\n",
      "loss: 38.466354  [14144/60000]\n",
      "loss: 74.983940  [14784/60000]\n",
      "loss: 26.797413  [15424/60000]\n",
      "loss: 157.363647  [16064/60000]\n",
      "loss: 142.712234  [16704/60000]\n",
      "loss: 131.992935  [17344/60000]\n",
      "loss: 155.285477  [17984/60000]\n",
      "loss: 75.881599  [18624/60000]\n",
      "loss: 124.516373  [19264/60000]\n",
      "loss: 135.657471  [19904/60000]\n",
      "loss: 54.027054  [20544/60000]\n",
      "loss: 85.133484  [21184/60000]\n",
      "loss: 363.770050  [21824/60000]\n",
      "loss: 92.228760  [22464/60000]\n",
      "loss: 112.880585  [23104/60000]\n",
      "loss: 82.348396  [23744/60000]\n",
      "loss: 127.517410  [24384/60000]\n",
      "loss: 84.165329  [25024/60000]\n",
      "loss: 115.431480  [25664/60000]\n",
      "loss: 221.917755  [26304/60000]\n",
      "loss: 111.563873  [26944/60000]\n",
      "loss: 126.886993  [27584/60000]\n",
      "loss: 93.694305  [28224/60000]\n",
      "loss: 49.155815  [28864/60000]\n",
      "loss: 131.603668  [29504/60000]\n",
      "loss: 234.328278  [30144/60000]\n",
      "loss: 113.625732  [30784/60000]\n",
      "loss: 66.284988  [31424/60000]\n",
      "loss: 131.200546  [32064/60000]\n",
      "loss: 89.392921  [32704/60000]\n",
      "loss: 53.070457  [33344/60000]\n",
      "loss: 118.320801  [33984/60000]\n",
      "loss: 37.680820  [34624/60000]\n",
      "loss: 31.569168  [35264/60000]\n",
      "loss: 76.720306  [35904/60000]\n",
      "loss: 123.715363  [36544/60000]\n",
      "loss: 201.893250  [37184/60000]\n",
      "loss: 65.897636  [37824/60000]\n",
      "loss: 143.857437  [38464/60000]\n",
      "loss: 72.611206  [39104/60000]\n",
      "loss: 115.285126  [39744/60000]\n",
      "loss: 88.353378  [40384/60000]\n",
      "loss: 128.266617  [41024/60000]\n",
      "loss: 111.394005  [41664/60000]\n",
      "loss: 138.795273  [42304/60000]\n",
      "loss: 102.860725  [42944/60000]\n",
      "loss: 91.399094  [43584/60000]\n",
      "loss: 218.809357  [44224/60000]\n",
      "loss: 150.431808  [44864/60000]\n",
      "loss: 89.150459  [45504/60000]\n",
      "loss: 106.981628  [46144/60000]\n",
      "loss: 111.332207  [46784/60000]\n",
      "loss: 51.524784  [47424/60000]\n",
      "loss: 76.996071  [48064/60000]\n",
      "loss: 94.843155  [48704/60000]\n",
      "loss: 74.706619  [49344/60000]\n",
      "loss: 113.869789  [49984/60000]\n",
      "loss: 73.398148  [50624/60000]\n",
      "loss: 232.757614  [51264/60000]\n",
      "loss: 219.012695  [51904/60000]\n",
      "loss: 323.270966  [52544/60000]\n",
      "loss: 137.246231  [53184/60000]\n",
      "loss: 52.991760  [53824/60000]\n",
      "loss: 73.980743  [54464/60000]\n",
      "loss: 86.631760  [55104/60000]\n",
      "loss: 68.348770  [55744/60000]\n",
      "loss: 100.916389  [56384/60000]\n",
      "loss: 90.098595  [57024/60000]\n",
      "loss: 134.645752  [57664/60000]\n",
      "loss: 38.178310  [58304/60000]\n",
      "loss: 60.462234  [58944/60000]\n",
      "loss: 204.825653  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 220.947226 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 150.532852  [   64/60000]\n",
      "loss: 86.014214  [  704/60000]\n",
      "loss: 61.273563  [ 1344/60000]\n",
      "loss: 159.965317  [ 1984/60000]\n",
      "loss: 100.125519  [ 2624/60000]\n",
      "loss: 88.943596  [ 3264/60000]\n",
      "loss: 132.514694  [ 3904/60000]\n",
      "loss: 48.029526  [ 4544/60000]\n",
      "loss: 149.604355  [ 5184/60000]\n",
      "loss: 60.813698  [ 5824/60000]\n",
      "loss: 57.899261  [ 6464/60000]\n",
      "loss: 82.215729  [ 7104/60000]\n",
      "loss: 243.111237  [ 7744/60000]\n",
      "loss: 153.798691  [ 8384/60000]\n",
      "loss: 65.089287  [ 9024/60000]\n",
      "loss: 120.364876  [ 9664/60000]\n",
      "loss: 204.186829  [10304/60000]\n",
      "loss: 70.051315  [10944/60000]\n",
      "loss: 181.100525  [11584/60000]\n",
      "loss: 239.321564  [12224/60000]\n",
      "loss: 454.617920  [12864/60000]\n",
      "loss: 139.696198  [13504/60000]\n",
      "loss: 37.121620  [14144/60000]\n",
      "loss: 101.950012  [14784/60000]\n",
      "loss: 48.014313  [15424/60000]\n",
      "loss: 157.322586  [16064/60000]\n",
      "loss: 128.517212  [16704/60000]\n",
      "loss: 149.367035  [17344/60000]\n",
      "loss: 110.968094  [17984/60000]\n",
      "loss: 74.614578  [18624/60000]\n",
      "loss: 86.334030  [19264/60000]\n",
      "loss: 174.103455  [19904/60000]\n",
      "loss: 50.792671  [20544/60000]\n",
      "loss: 70.225746  [21184/60000]\n",
      "loss: 319.899506  [21824/60000]\n",
      "loss: 101.965889  [22464/60000]\n",
      "loss: 188.692841  [23104/60000]\n",
      "loss: 73.606415  [23744/60000]\n",
      "loss: 101.718391  [24384/60000]\n",
      "loss: 82.807213  [25024/60000]\n",
      "loss: 83.621979  [25664/60000]\n",
      "loss: 183.904602  [26304/60000]\n",
      "loss: 91.905106  [26944/60000]\n",
      "loss: 154.159729  [27584/60000]\n",
      "loss: 47.157955  [28224/60000]\n",
      "loss: 30.672163  [28864/60000]\n",
      "loss: 111.101929  [29504/60000]\n",
      "loss: 130.621704  [30144/60000]\n",
      "loss: 135.775879  [30784/60000]\n",
      "loss: 25.713308  [31424/60000]\n",
      "loss: 83.206146  [32064/60000]\n",
      "loss: 143.964874  [32704/60000]\n",
      "loss: 40.114059  [33344/60000]\n",
      "loss: 156.909470  [33984/60000]\n",
      "loss: 43.611828  [34624/60000]\n",
      "loss: 52.711349  [35264/60000]\n",
      "loss: 75.500496  [35904/60000]\n",
      "loss: 96.088356  [36544/60000]\n",
      "loss: 231.272980  [37184/60000]\n",
      "loss: 51.794918  [37824/60000]\n",
      "loss: 94.605202  [38464/60000]\n",
      "loss: 115.914085  [39104/60000]\n",
      "loss: 135.317780  [39744/60000]\n",
      "loss: 91.621414  [40384/60000]\n",
      "loss: 45.963425  [41024/60000]\n",
      "loss: 113.628845  [41664/60000]\n",
      "loss: 124.485054  [42304/60000]\n",
      "loss: 56.418976  [42944/60000]\n",
      "loss: 41.504726  [43584/60000]\n",
      "loss: 158.683609  [44224/60000]\n",
      "loss: 127.670792  [44864/60000]\n",
      "loss: 54.373383  [45504/60000]\n",
      "loss: 69.346222  [46144/60000]\n",
      "loss: 162.358566  [46784/60000]\n",
      "loss: 42.036453  [47424/60000]\n",
      "loss: 89.941498  [48064/60000]\n",
      "loss: 81.658142  [48704/60000]\n",
      "loss: 105.016098  [49344/60000]\n",
      "loss: 114.447983  [49984/60000]\n",
      "loss: 56.917988  [50624/60000]\n",
      "loss: 211.251251  [51264/60000]\n",
      "loss: 341.704407  [51904/60000]\n",
      "loss: 111.922546  [52544/60000]\n",
      "loss: 62.419807  [53184/60000]\n",
      "loss: 64.298767  [53824/60000]\n",
      "loss: 191.119049  [54464/60000]\n",
      "loss: 34.733543  [55104/60000]\n",
      "loss: 65.016418  [55744/60000]\n",
      "loss: 138.104309  [56384/60000]\n",
      "loss: 74.416550  [57024/60000]\n",
      "loss: 163.885513  [57664/60000]\n",
      "loss: 60.522762  [58304/60000]\n",
      "loss: 71.446167  [58944/60000]\n",
      "loss: 198.593964  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 106.320320 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 55.999706  [   64/60000]\n",
      "loss: 62.759121  [  704/60000]\n",
      "loss: 115.119812  [ 1344/60000]\n",
      "loss: 67.796844  [ 1984/60000]\n",
      "loss: 126.910110  [ 2624/60000]\n",
      "loss: 83.247025  [ 3264/60000]\n",
      "loss: 127.919724  [ 3904/60000]\n",
      "loss: 72.045334  [ 4544/60000]\n",
      "loss: 227.117416  [ 5184/60000]\n",
      "loss: 141.886871  [ 5824/60000]\n",
      "loss: 36.285587  [ 6464/60000]\n",
      "loss: 109.990715  [ 7104/60000]\n",
      "loss: 282.684570  [ 7744/60000]\n",
      "loss: 195.149124  [ 8384/60000]\n",
      "loss: 105.833267  [ 9024/60000]\n",
      "loss: 284.528748  [ 9664/60000]\n",
      "loss: 57.332226  [10304/60000]\n",
      "loss: 72.732773  [10944/60000]\n",
      "loss: 99.419434  [11584/60000]\n",
      "loss: 176.988251  [12224/60000]\n",
      "loss: 118.926064  [12864/60000]\n",
      "loss: 100.645302  [13504/60000]\n",
      "loss: 41.430077  [14144/60000]\n",
      "loss: 71.357597  [14784/60000]\n",
      "loss: 50.187050  [15424/60000]\n",
      "loss: 108.761009  [16064/60000]\n",
      "loss: 146.019836  [16704/60000]\n",
      "loss: 123.124939  [17344/60000]\n",
      "loss: 98.240364  [17984/60000]\n",
      "loss: 90.158508  [18624/60000]\n",
      "loss: 106.898376  [19264/60000]\n",
      "loss: 146.738739  [19904/60000]\n",
      "loss: 55.879539  [20544/60000]\n",
      "loss: 91.432961  [21184/60000]\n",
      "loss: 193.492844  [21824/60000]\n",
      "loss: 112.668152  [22464/60000]\n",
      "loss: 112.841904  [23104/60000]\n",
      "loss: 95.973114  [23744/60000]\n",
      "loss: 246.701172  [24384/60000]\n",
      "loss: 82.849457  [25024/60000]\n",
      "loss: 87.075935  [25664/60000]\n",
      "loss: 186.381760  [26304/60000]\n",
      "loss: 120.617218  [26944/60000]\n",
      "loss: 215.120850  [27584/60000]\n",
      "loss: 57.382019  [28224/60000]\n",
      "loss: 158.178452  [28864/60000]\n",
      "loss: 134.775238  [29504/60000]\n",
      "loss: 161.029831  [30144/60000]\n",
      "loss: 88.907135  [30784/60000]\n",
      "loss: 69.640343  [31424/60000]\n",
      "loss: 80.890083  [32064/60000]\n",
      "loss: 127.942276  [32704/60000]\n",
      "loss: 71.531677  [33344/60000]\n",
      "loss: 118.864281  [33984/60000]\n",
      "loss: 43.723682  [34624/60000]\n",
      "loss: 35.171219  [35264/60000]\n",
      "loss: 82.794411  [35904/60000]\n",
      "loss: 72.871376  [36544/60000]\n",
      "loss: 188.695541  [37184/60000]\n",
      "loss: 68.518112  [37824/60000]\n",
      "loss: 86.131920  [38464/60000]\n",
      "loss: 45.005234  [39104/60000]\n",
      "loss: 150.811279  [39744/60000]\n",
      "loss: 102.715790  [40384/60000]\n",
      "loss: 183.529617  [41024/60000]\n",
      "loss: 114.738937  [41664/60000]\n",
      "loss: 132.626999  [42304/60000]\n",
      "loss: 321.219910  [42944/60000]\n",
      "loss: 264.832397  [43584/60000]\n",
      "loss: 148.015488  [44224/60000]\n",
      "loss: 135.488892  [44864/60000]\n",
      "loss: 99.375244  [45504/60000]\n",
      "loss: 146.363007  [46144/60000]\n",
      "loss: 131.004715  [46784/60000]\n",
      "loss: 38.116734  [47424/60000]\n",
      "loss: 108.795074  [48064/60000]\n",
      "loss: 106.253349  [48704/60000]\n",
      "loss: 67.366844  [49344/60000]\n",
      "loss: 152.919479  [49984/60000]\n",
      "loss: 44.162796  [50624/60000]\n",
      "loss: 116.253868  [51264/60000]\n",
      "loss: 139.074585  [51904/60000]\n",
      "loss: 352.935486  [52544/60000]\n",
      "loss: 253.363541  [53184/60000]\n",
      "loss: 50.024574  [53824/60000]\n",
      "loss: 99.831047  [54464/60000]\n",
      "loss: 58.218880  [55104/60000]\n",
      "loss: 67.079643  [55744/60000]\n",
      "loss: 73.986183  [56384/60000]\n",
      "loss: 84.747467  [57024/60000]\n",
      "loss: 127.617386  [57664/60000]\n",
      "loss: 102.073074  [58304/60000]\n",
      "loss: 74.846596  [58944/60000]\n",
      "loss: 258.898254  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 139.722683 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 80.321388  [   64/60000]\n",
      "loss: 136.003723  [  704/60000]\n",
      "loss: 50.422909  [ 1344/60000]\n",
      "loss: 159.284424  [ 1984/60000]\n",
      "loss: 110.544121  [ 2624/60000]\n",
      "loss: 93.270531  [ 3264/60000]\n",
      "loss: 149.729172  [ 3904/60000]\n",
      "loss: 51.103981  [ 4544/60000]\n",
      "loss: 414.317902  [ 5184/60000]\n",
      "loss: 132.068054  [ 5824/60000]\n",
      "loss: 41.656090  [ 6464/60000]\n",
      "loss: 185.423431  [ 7104/60000]\n",
      "loss: 183.835938  [ 7744/60000]\n",
      "loss: 141.804703  [ 8384/60000]\n",
      "loss: 105.674133  [ 9024/60000]\n",
      "loss: 497.169006  [ 9664/60000]\n",
      "loss: 105.378349  [10304/60000]\n",
      "loss: 57.274593  [10944/60000]\n",
      "loss: 98.494995  [11584/60000]\n",
      "loss: 199.985748  [12224/60000]\n",
      "loss: 168.673462  [12864/60000]\n",
      "loss: 117.934593  [13504/60000]\n",
      "loss: 45.299664  [14144/60000]\n",
      "loss: 100.637642  [14784/60000]\n",
      "loss: 17.394917  [15424/60000]\n",
      "loss: 115.868301  [16064/60000]\n",
      "loss: 46.494720  [16704/60000]\n",
      "loss: 128.323975  [17344/60000]\n",
      "loss: 78.835670  [17984/60000]\n",
      "loss: 95.381622  [18624/60000]\n",
      "loss: 161.537186  [19264/60000]\n",
      "loss: 134.892288  [19904/60000]\n",
      "loss: 59.966396  [20544/60000]\n",
      "loss: 97.481232  [21184/60000]\n",
      "loss: 271.365509  [21824/60000]\n",
      "loss: 68.555992  [22464/60000]\n",
      "loss: 164.920578  [23104/60000]\n",
      "loss: 70.321121  [23744/60000]\n",
      "loss: 90.824783  [24384/60000]\n",
      "loss: 77.440842  [25024/60000]\n",
      "loss: 100.834625  [25664/60000]\n",
      "loss: 179.418030  [26304/60000]\n",
      "loss: 94.730865  [26944/60000]\n",
      "loss: 166.553314  [27584/60000]\n",
      "loss: 58.669205  [28224/60000]\n",
      "loss: 34.493240  [28864/60000]\n",
      "loss: 113.978653  [29504/60000]\n",
      "loss: 148.557465  [30144/60000]\n",
      "loss: 63.913826  [30784/60000]\n",
      "loss: 47.683983  [31424/60000]\n",
      "loss: 88.785194  [32064/60000]\n",
      "loss: 69.617966  [32704/60000]\n",
      "loss: 59.810028  [33344/60000]\n",
      "loss: 172.725189  [33984/60000]\n",
      "loss: 112.034538  [34624/60000]\n",
      "loss: 105.372787  [35264/60000]\n",
      "loss: 96.768661  [35904/60000]\n",
      "loss: 88.969917  [36544/60000]\n",
      "loss: 248.319580  [37184/60000]\n",
      "loss: 32.991089  [37824/60000]\n",
      "loss: 86.442665  [38464/60000]\n",
      "loss: 149.810226  [39104/60000]\n",
      "loss: 153.688751  [39744/60000]\n",
      "loss: 101.476585  [40384/60000]\n",
      "loss: 195.745468  [41024/60000]\n",
      "loss: 148.344681  [41664/60000]\n",
      "loss: 125.382477  [42304/60000]\n",
      "loss: 82.184113  [42944/60000]\n",
      "loss: 55.353806  [43584/60000]\n",
      "loss: 225.099091  [44224/60000]\n",
      "loss: 150.577072  [44864/60000]\n",
      "loss: 97.354218  [45504/60000]\n",
      "loss: 70.169861  [46144/60000]\n",
      "loss: 85.838875  [46784/60000]\n",
      "loss: 41.682346  [47424/60000]\n",
      "loss: 106.887161  [48064/60000]\n",
      "loss: 94.358612  [48704/60000]\n",
      "loss: 138.163971  [49344/60000]\n",
      "loss: 125.353981  [49984/60000]\n",
      "loss: 59.500164  [50624/60000]\n",
      "loss: 182.623550  [51264/60000]\n",
      "loss: 274.420349  [51904/60000]\n",
      "loss: 247.833649  [52544/60000]\n",
      "loss: 206.526947  [53184/60000]\n",
      "loss: 84.919769  [53824/60000]\n",
      "loss: 203.757599  [54464/60000]\n",
      "loss: 53.677734  [55104/60000]\n",
      "loss: 65.170082  [55744/60000]\n",
      "loss: 87.637955  [56384/60000]\n",
      "loss: 76.793686  [57024/60000]\n",
      "loss: 102.038361  [57664/60000]\n",
      "loss: 25.309702  [58304/60000]\n",
      "loss: 73.745590  [58944/60000]\n",
      "loss: 222.171799  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 168.937970 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 109.809853  [   64/60000]\n",
      "loss: 63.034351  [  704/60000]\n",
      "loss: 120.269897  [ 1344/60000]\n",
      "loss: 126.775421  [ 1984/60000]\n",
      "loss: 104.556488  [ 2624/60000]\n",
      "loss: 95.654564  [ 3264/60000]\n",
      "loss: 118.300743  [ 3904/60000]\n",
      "loss: 73.711044  [ 4544/60000]\n",
      "loss: 132.838501  [ 5184/60000]\n",
      "loss: 199.731857  [ 5824/60000]\n",
      "loss: 50.918690  [ 6464/60000]\n",
      "loss: 153.828888  [ 7104/60000]\n",
      "loss: 208.025528  [ 7744/60000]\n",
      "loss: 288.358215  [ 8384/60000]\n",
      "loss: 129.309952  [ 9024/60000]\n",
      "loss: 256.843262  [ 9664/60000]\n",
      "loss: 57.071281  [10304/60000]\n",
      "loss: 53.797817  [10944/60000]\n",
      "loss: 102.974297  [11584/60000]\n",
      "loss: 245.682068  [12224/60000]\n",
      "loss: 235.073853  [12864/60000]\n",
      "loss: 185.907150  [13504/60000]\n",
      "loss: 43.214386  [14144/60000]\n",
      "loss: 90.704811  [14784/60000]\n",
      "loss: 24.126165  [15424/60000]\n",
      "loss: 146.286774  [16064/60000]\n",
      "loss: 124.061844  [16704/60000]\n",
      "loss: 132.703781  [17344/60000]\n",
      "loss: 83.058105  [17984/60000]\n",
      "loss: 53.092739  [18624/60000]\n",
      "loss: 145.112976  [19264/60000]\n",
      "loss: 150.102753  [19904/60000]\n",
      "loss: 62.335930  [20544/60000]\n",
      "loss: 105.793396  [21184/60000]\n",
      "loss: 389.177490  [21824/60000]\n",
      "loss: 157.360016  [22464/60000]\n",
      "loss: 71.374718  [23104/60000]\n",
      "loss: 85.896530  [23744/60000]\n",
      "loss: 97.590240  [24384/60000]\n",
      "loss: 117.799553  [25024/60000]\n",
      "loss: 177.813721  [25664/60000]\n",
      "loss: 223.327057  [26304/60000]\n",
      "loss: 132.062042  [26944/60000]\n",
      "loss: 155.024002  [27584/60000]\n",
      "loss: 64.356308  [28224/60000]\n",
      "loss: 39.937057  [28864/60000]\n",
      "loss: 218.312256  [29504/60000]\n",
      "loss: 160.156357  [30144/60000]\n",
      "loss: 94.569824  [30784/60000]\n",
      "loss: 47.043499  [31424/60000]\n",
      "loss: 74.998299  [32064/60000]\n",
      "loss: 72.088837  [32704/60000]\n",
      "loss: 47.944229  [33344/60000]\n",
      "loss: 114.947067  [33984/60000]\n",
      "loss: 48.684471  [34624/60000]\n",
      "loss: 58.661446  [35264/60000]\n",
      "loss: 119.507530  [35904/60000]\n",
      "loss: 76.043884  [36544/60000]\n",
      "loss: 240.024277  [37184/60000]\n",
      "loss: 36.274296  [37824/60000]\n",
      "loss: 134.671280  [38464/60000]\n",
      "loss: 136.193832  [39104/60000]\n",
      "loss: 103.759323  [39744/60000]\n",
      "loss: 115.793564  [40384/60000]\n",
      "loss: 69.840538  [41024/60000]\n",
      "loss: 180.049789  [41664/60000]\n",
      "loss: 109.362747  [42304/60000]\n",
      "loss: 127.740509  [42944/60000]\n",
      "loss: 88.468727  [43584/60000]\n",
      "loss: 233.867676  [44224/60000]\n",
      "loss: 82.716087  [44864/60000]\n",
      "loss: 76.036392  [45504/60000]\n",
      "loss: 54.524357  [46144/60000]\n",
      "loss: 122.428505  [46784/60000]\n",
      "loss: 39.082714  [47424/60000]\n",
      "loss: 103.835732  [48064/60000]\n",
      "loss: 100.674049  [48704/60000]\n",
      "loss: 71.095901  [49344/60000]\n",
      "loss: 149.952850  [49984/60000]\n",
      "loss: 61.435944  [50624/60000]\n",
      "loss: 199.020554  [51264/60000]\n",
      "loss: 331.858978  [51904/60000]\n",
      "loss: 276.525665  [52544/60000]\n",
      "loss: 218.928284  [53184/60000]\n",
      "loss: 54.867016  [53824/60000]\n",
      "loss: 104.021790  [54464/60000]\n",
      "loss: 77.431564  [55104/60000]\n",
      "loss: 79.535294  [55744/60000]\n",
      "loss: 136.511246  [56384/60000]\n",
      "loss: 92.469086  [57024/60000]\n",
      "loss: 153.736618  [57664/60000]\n",
      "loss: 25.312519  [58304/60000]\n",
      "loss: 65.651489  [58944/60000]\n",
      "loss: 177.715424  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 320.266541 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 265.884827  [   64/60000]\n",
      "loss: 95.751984  [  704/60000]\n",
      "loss: 135.190140  [ 1344/60000]\n",
      "loss: 187.804047  [ 1984/60000]\n",
      "loss: 122.924332  [ 2624/60000]\n",
      "loss: 86.242279  [ 3264/60000]\n",
      "loss: 94.006699  [ 3904/60000]\n",
      "loss: 51.076996  [ 4544/60000]\n",
      "loss: 244.967941  [ 5184/60000]\n",
      "loss: 56.295444  [ 5824/60000]\n",
      "loss: 48.109043  [ 6464/60000]\n",
      "loss: 135.944397  [ 7104/60000]\n",
      "loss: 180.521027  [ 7744/60000]\n",
      "loss: 87.484634  [ 8384/60000]\n",
      "loss: 89.512787  [ 9024/60000]\n",
      "loss: 259.634705  [ 9664/60000]\n",
      "loss: 57.117958  [10304/60000]\n",
      "loss: 98.221603  [10944/60000]\n",
      "loss: 120.094666  [11584/60000]\n",
      "loss: 459.993988  [12224/60000]\n",
      "loss: 182.415512  [12864/60000]\n",
      "loss: 98.230400  [13504/60000]\n",
      "loss: 57.272827  [14144/60000]\n",
      "loss: 119.683838  [14784/60000]\n",
      "loss: 20.114872  [15424/60000]\n",
      "loss: 71.008064  [16064/60000]\n",
      "loss: 41.869999  [16704/60000]\n",
      "loss: 130.578979  [17344/60000]\n",
      "loss: 90.944611  [17984/60000]\n",
      "loss: 72.244087  [18624/60000]\n",
      "loss: 165.432739  [19264/60000]\n",
      "loss: 155.146515  [19904/60000]\n",
      "loss: 117.544312  [20544/60000]\n",
      "loss: 92.526138  [21184/60000]\n",
      "loss: 323.352814  [21824/60000]\n",
      "loss: 138.917969  [22464/60000]\n",
      "loss: 69.313309  [23104/60000]\n",
      "loss: 68.058716  [23744/60000]\n",
      "loss: 97.160629  [24384/60000]\n",
      "loss: 104.745010  [25024/60000]\n",
      "loss: 84.099640  [25664/60000]\n",
      "loss: 224.790573  [26304/60000]\n",
      "loss: 128.645721  [26944/60000]\n",
      "loss: 181.071716  [27584/60000]\n",
      "loss: 60.138523  [28224/60000]\n",
      "loss: 41.080395  [28864/60000]\n",
      "loss: 134.273956  [29504/60000]\n",
      "loss: 130.136810  [30144/60000]\n",
      "loss: 81.992294  [30784/60000]\n",
      "loss: 59.011799  [31424/60000]\n",
      "loss: 118.037956  [32064/60000]\n",
      "loss: 104.993591  [32704/60000]\n",
      "loss: 52.406086  [33344/60000]\n",
      "loss: 124.235611  [33984/60000]\n",
      "loss: 41.062462  [34624/60000]\n",
      "loss: 37.247574  [35264/60000]\n",
      "loss: 67.166595  [35904/60000]\n",
      "loss: 106.554070  [36544/60000]\n",
      "loss: 259.324768  [37184/60000]\n",
      "loss: 50.429707  [37824/60000]\n",
      "loss: 155.176208  [38464/60000]\n",
      "loss: 123.548775  [39104/60000]\n",
      "loss: 154.007645  [39744/60000]\n",
      "loss: 166.579269  [40384/60000]\n",
      "loss: 184.701202  [41024/60000]\n",
      "loss: 100.500290  [41664/60000]\n",
      "loss: 167.607727  [42304/60000]\n",
      "loss: 82.619423  [42944/60000]\n",
      "loss: 200.189880  [43584/60000]\n",
      "loss: 211.000458  [44224/60000]\n",
      "loss: 131.766754  [44864/60000]\n",
      "loss: 80.967453  [45504/60000]\n",
      "loss: 211.191147  [46144/60000]\n",
      "loss: 128.835739  [46784/60000]\n",
      "loss: 93.833771  [47424/60000]\n",
      "loss: 101.049698  [48064/60000]\n",
      "loss: 71.461472  [48704/60000]\n",
      "loss: 76.774315  [49344/60000]\n",
      "loss: 188.943298  [49984/60000]\n",
      "loss: 69.346756  [50624/60000]\n",
      "loss: 129.412292  [51264/60000]\n",
      "loss: 163.181076  [51904/60000]\n",
      "loss: 87.459747  [52544/60000]\n",
      "loss: 175.380219  [53184/60000]\n",
      "loss: 55.453613  [53824/60000]\n",
      "loss: 163.765411  [54464/60000]\n",
      "loss: 44.157627  [55104/60000]\n",
      "loss: 104.014923  [55744/60000]\n",
      "loss: 126.322037  [56384/60000]\n",
      "loss: 90.899414  [57024/60000]\n",
      "loss: 122.135391  [57664/60000]\n",
      "loss: 21.103672  [58304/60000]\n",
      "loss: 46.114510  [58944/60000]\n",
      "loss: 159.153305  [59584/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 199.428761 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#now we train our model with a for loop\n",
    "\n",
    "# Need to repeat the training process for each epoch.\n",
    "#   In each epoch, the model will eventually see EVERY\n",
    "#   observations in the data\n",
    "for t in range(epochs): #trains once for every epoch,\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer) #run our trainig loop through the entire data set\n",
    "    test_loop(test_dataloader, model, loss_fn) #then run 1 test loop, then it will move on to the next epch\n",
    "print(\"Done!\") #print out each poc\n",
    "\n",
    "#careful because if you run this again it will continue to update the model! (it will start at where you stopped last time (from the last epoch you stopped on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b38b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is predicted to be a Shirt, and is labeled as Pullover\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#make more predictions\n",
    "\n",
    "# Decide if we are loading for predictions or more training\n",
    "model.eval()\n",
    "# - or -\n",
    "# model.train()\n",
    "\n",
    "# Make predictions\n",
    "pred = model(test_data.__getitem__(1)[0]).argmax()\n",
    "truth = test_data.__getitem__(1)[1]\n",
    "\n",
    "#now switch theses so that they give the text label (instead of a number)\n",
    "\n",
    "pred_text = labels_map[pred.item()]\n",
    "truth_text = labels_map[truth]\n",
    "\n",
    "#print(f\"This image is predicted to be a {pred}, and is labeled as {truth}\") #this one gives numbers\n",
    "\n",
    "print(f\"This image is predicted to be a {pred_text}, and is labeled as {truth_text}\")\n",
    "\n",
    "#our model so far is still like 40 years out of date lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39d09171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving a model, importing  and continuing to update it from there \n",
    "#this matters because energy consuptions matters! Very wasteful from an environemntal and cost perspective to do create these models over and over\n",
    "#thus its important to save our models so we can use them later!\n",
    "\n",
    "# Save our model for later, so we can train more or make predictions\n",
    "\n",
    "EPOCH = epochs #stores our epoch\n",
    "# We use the .pt file extension by convention for saving\n",
    "#    pytorch models\n",
    "PATH = \"model.pt\" #create a psth and then save the moedl\n",
    "\n",
    "# The save function creates a binary storing all our data for us\n",
    "torch.save({ #pass in a dictionary of all the stuff you want to save\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, PATH) #path is the file were going to save this info into \n",
    "\n",
    "            #save weights, biases, as well as epochs and optimozers into the file\n",
    "\n",
    "#should be able to find saved file over in the files now, (will be model.pt in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d71904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now to load back in our model in our next session we do the following: \n",
    "\n",
    "# Specify our path\n",
    "PATH = \"model.pt\"\n",
    "\n",
    "# Create a new \"blank\" model to load our information into (has no updated weights and biases)\n",
    "model = FirstNet()\n",
    "\n",
    "# Recreate our optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9) #(has no updated weights and biases)\n",
    "\n",
    "# Load back all of our data from the file (puts all that stuff back in mmeory, this means we dont have to train the model everytime we use it)\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "EPOCH = checkpoint['epoch']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
